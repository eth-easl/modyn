diff --git a/PyTorch/Recommendation/DLRM/preproc/DGX-A100_config.sh b/PyTorch/Recommendation/DLRM/preproc/DGX-A100_config.sh
index ecb4e865..6f6b4dd9 100644
--- a/PyTorch/Recommendation/DLRM/preproc/DGX-A100_config.sh
+++ b/PyTorch/Recommendation/DLRM/preproc/DGX-A100_config.sh
@@ -20,19 +20,25 @@

 # below numbers should be adjusted according to the resource of your running environment
 # set the total number of CPU cores, spark can use
-export TOTAL_CORES=256
+
+# MODYN:
+# Note this is a working setup for the 1 GPU setup with a n1-18-standard VM with 32gig. Edit it if
+# the config is different. The cores and executors were setup based on working runs, while the memory
+# was setup to fit within the 32GB of the VM used.
+# TODO(#156): Update this config to optimally use the resources of the chosen VM and explain the updates made.
+export TOTAL_CORES=1

 # set the number of executors
-export NUM_EXECUTORS=8
+export NUM_EXECUTORS=1

 # the cores for each executor, it'll be calculated
 export NUM_EXECUTOR_CORES=$((${TOTAL_CORES}/${NUM_EXECUTORS}))

 # unit: GB,  set the max memory you want to use
-export TOTAL_MEMORY=2000
+export TOTAL_MEMORY=29

 # unit: GB, set the memory for driver
-export DRIVER_MEMORY=32
+export DRIVER_MEMORY=5

 # the memory per executor
-export EXECUTOR_MEMORY=$(((${TOTAL_MEMORY}-${DRIVER_MEMORY})/${NUM_EXECUTORS}-16))
+export EXECUTOR_MEMORY=$(((${TOTAL_MEMORY}-${DRIVER_MEMORY})/${NUM_EXECUTORS}))
diff --git a/PyTorch/Recommendation/DLRM/preproc/parquet_to_binary.py b/PyTorch/Recommendation/DLRM/preproc/parquet_to_binary.py
index f71255e8..af3b09f9 100644
--- a/PyTorch/Recommendation/DLRM/preproc/parquet_to_binary.py
+++ b/PyTorch/Recommendation/DLRM/preproc/parquet_to_binary.py
@@ -45,47 +45,20 @@ def process_file(f, dst):
 def main():
     parser = argparse.ArgumentParser()
     parser.add_argument('--src_dir', type=str)
-    parser.add_argument('--intermediate_dir', type=str)
     parser.add_argument('--dst_dir', type=str)
     parser.add_argument('--parallel_jobs', default=40, type=int)
     args = parser.parse_args()

-    print('Processing train files...')
-    train_src_files = glob.glob(args.src_dir + '/train/*.parquet')
-    train_intermediate_dir = os.path.join(args.intermediate_dir, 'train')
-    os.makedirs(train_intermediate_dir, exist_ok=True)
+    for i in range(24):
+        day = 'day'+str(i)
+        print('Processing', day ,' files...')
+        day_src_files = glob.glob(args.src_dir + '/' + day + '/*.parquet')
+        day_dest_dir = os.path.join(args.dst_dir, day)
+        os.makedirs(day_dest_dir, exist_ok=True)

-    Parallel(n_jobs=args.parallel_jobs)(delayed(process_file)(f, train_intermediate_dir) for f in tqdm.tqdm(train_src_files))
+        Parallel(n_jobs=args.parallel_jobs)(delayed(process_file)(f, day_dest_dir) for f in tqdm.tqdm(day_src_files))

-    print('Train files conversion done')
-
-    print('Processing test files...')
-    test_src_files = glob.glob(args.src_dir + '/test/*.parquet')
-    test_intermediate_dir = os.path.join(args.intermediate_dir, 'test')
-    os.makedirs(test_intermediate_dir, exist_ok=True)
-
-    Parallel(n_jobs=args.parallel_jobs)(delayed(process_file)(f, test_intermediate_dir) for f in tqdm.tqdm(test_src_files))
-    print('Test files conversion done')
-
-    print('Processing validation files...')
-    valid_src_files = glob.glob(args.src_dir + '/validation/*.parquet')
-    valid_intermediate_dir = os.path.join(args.intermediate_dir, 'validation')
-    os.makedirs(valid_intermediate_dir, exist_ok=True)
-
-    Parallel(n_jobs=args.parallel_jobs)(delayed(process_file)(f, valid_intermediate_dir) for f in tqdm.tqdm(valid_src_files))
-    print('Validation files conversion done')
-
-    os.makedirs(args.dst_dir, exist_ok=True)
-
-    print('Concatenating train files')
-    os.system(f'cat {train_intermediate_dir}/*.bin > {args.dst_dir}/train_data.bin')
-
-    print('Concatenating test files')
-    os.system(f'cat {test_intermediate_dir}/*.bin > {args.dst_dir}/test_data.bin')
-
-    print('Concatenating validation files')
-    os.system(f'cat {valid_intermediate_dir}/*.bin > {args.dst_dir}/validation_data.bin')
-    print('Done')
+        print(day, 'files conversion done')


 if __name__ == '__main__':
diff --git a/PyTorch/Recommendation/DLRM/preproc/prepare_dataset.sh b/PyTorch/Recommendation/DLRM/preproc/prepare_dataset.sh
index 6b995d0a..36aabfe4 100644
--- a/PyTorch/Recommendation/DLRM/preproc/prepare_dataset.sh
+++ b/PyTorch/Recommendation/DLRM/preproc/prepare_dataset.sh
@@ -60,33 +60,13 @@ else
     preprocessing_version=Spark
 fi

-conversion_intermediate_dir=${conversion_intermediate_dir:-'/data/dlrm/intermediate_binary'}
 final_output_dir=${final_output_dir:-'/data/dlrm/binary_dataset'}

 source ${DGX_VERSION}_config.sh

-if [ -d ${final_output_dir}/train ] \
-   && [ -d ${final_output_dir}/validation ] \
-   && [ -d ${final_output_dir}/test ] \
-   && [ -f ${final_output_dir}/feature_spec.yaml ]; then
+echo "Performing final conversion to a custom data format"
+python parquet_to_binary.py --parallel_jobs ${TOTAL_CORES} --src_dir ${output_path} \
+                            --dst_dir ${final_output_dir}
+cp "${output_path}/model_size.json" "${final_output_dir}/model_size.json"

-    echo "Final conversion already done"
-else
-    echo "Performing final conversion to a custom data format"
-    python parquet_to_binary.py --parallel_jobs ${TOTAL_CORES} --src_dir ${output_path} \
-                                --intermediate_dir  ${conversion_intermediate_dir} \
-                                --dst_dir ${final_output_dir}
-
-    cp "${output_path}/model_size.json" "${final_output_dir}/model_size.json"
-
-    python split_dataset.py --dataset "${final_output_dir}" --output "${final_output_dir}/split"
-    rm ${final_output_dir}/train_data.bin
-    rm ${final_output_dir}/validation_data.bin
-    rm ${final_output_dir}/test_data.bin
-    rm ${final_output_dir}/model_size.json
-
-    mv ${final_output_dir}/split/* ${final_output_dir}
-    rm -rf ${final_output_dir}/split
-fi
-
-echo "Done preprocessing the Criteo Kaggle Dataset"
+echo "Done preprocessing the Criteo Kaggle Dataset"
\ No newline at end of file
diff --git a/PyTorch/Recommendation/DLRM/preproc/run_spark_gpu_DGX-A100.sh b/PyTorch/Recommendation/DLRM/preproc/run_spark_gpu_DGX-A100.sh
index a38d5214..0bfc3266 100644
--- a/PyTorch/Recommendation/DLRM/preproc/run_spark_gpu_DGX-A100.sh
+++ b/PyTorch/Recommendation/DLRM/preproc/run_spark_gpu_DGX-A100.sh
@@ -79,13 +79,15 @@ spark-submit --master $MASTER \
     --model_folder $OUTPUT_PATH/models \
     --write_mode overwrite --low_mem 2>&1 | tee submit_dict_log.txt

-echo "Transforming the train data from day_0 to day_22..."
-spark-submit --master $MASTER \
+for i in {0..23}
+do
+   echo "Transforming the data for day_${i}..."
+   spark-submit --master $MASTER \
     --driver-memory "${DRIVER_MEMORY}G" \
     --executor-cores $NUM_EXECUTOR_CORES \
     --executor-memory "${EXECUTOR_MEMORY}G" \
     --conf spark.cores.max=$TOTAL_CORES \
-    --conf spark.task.cpus=16 \
+    --conf spark.task.cpus=1 \
     --conf spark.sql.files.maxPartitionBytes=1073741824 \
     --conf spark.sql.shuffle.partitions=600 \
     --conf spark.driver.maxResultSize=2G \
@@ -104,87 +106,12 @@ spark-submit --master $MASTER \
     --conf spark.executor.extraJavaOptions="-Dcom.nvidia.cudf.prefer-pinned=true\ -Djava.io.tmpdir=$SPARK_LOCAL_DIRS" \
     spark_data_utils.py --mode transform \
     --input_folder $INPUT_PATH \
-    --days 0-22 \
-    --output_folder $OUTPUT_PATH/train \
+    --days ${i}-${i} \
+    --output_folder $OUTPUT_PATH/day${i} \
     --model_size_file $OUTPUT_PATH/model_size.json \
     --model_folder $OUTPUT_PATH/models \
-    --write_mode overwrite --low_mem 2>&1 | tee submit_train_log.txt
-
-echo "Splitting the last day into 2 parts of test and validation..."
-last_day=$INPUT_PATH/day_23
-temp_test=$OUTPUT_PATH/temp/test
-temp_validation=$OUTPUT_PATH/temp/validation
-mkdir -p $temp_test $temp_validation
-
-lines=`wc -l $last_day | awk '{print $1}'`
-former=$((lines / 2))
-latter=$((lines - former))
-
-head -n $former $last_day > $temp_test/day_23
-tail -n $latter $last_day > $temp_validation/day_23
-
-echo "Transforming the test data in day_23..."
-spark-submit --master $MASTER \
-    --driver-memory "${DRIVER_MEMORY}G" \
-    --executor-cores $NUM_EXECUTOR_CORES \
-    --executor-memory "${EXECUTOR_MEMORY}G" \
-    --conf spark.cores.max=$TOTAL_CORES \
-    --conf spark.task.cpus=32 \
-    --conf spark.sql.files.maxPartitionBytes=1073741824 \
-    --conf spark.sql.shuffle.partitions=600 \
-    --conf spark.driver.maxResultSize=2G \
-    --conf spark.locality.wait=0s \
-    --conf spark.network.timeout=1800s \
-    --conf spark.task.resource.gpu.amount=1 \
-    --conf spark.executor.resource.gpu.amount=1 \
-    --conf spark.plugins=com.nvidia.spark.SQLPlugin \
-    --conf spark.rapids.sql.concurrentGpuTasks=1 \
-    --conf spark.rapids.sql.reader.batchSizeRows=4000000 \
-    --conf spark.rapids.memory.pinnedPool.size=16g \
-    --conf spark.rapids.sql.explain=ALL \
-    --conf spark.sql.autoBroadcastJoinThreshold=1GB \
-    --conf spark.rapids.sql.incompatibleOps.enabled=true \
-    --conf spark.driver.maxResultSize=2G \
-    --conf spark.executor.extraJavaOptions="-Dcom.nvidia.cudf.prefer-pinned=true\ -Djava.io.tmpdir=$SPARK_LOCAL_DIRS" \
-    spark_data_utils.py --mode transform \
-    --input_folder $temp_test \
-    --days 23-23 \
-    --output_folder $OUTPUT_PATH/test \
-    --output_ordering input \
-    --model_folder $OUTPUT_PATH/models \
-    --write_mode overwrite --low_mem 2>&1 | tee submit_test_log.txt
-
-echo "Transforming the validation data in day_23..."
-spark-submit --master $MASTER \
-    --driver-memory "${DRIVER_MEMORY}G" \
-    --executor-cores $NUM_EXECUTOR_CORES \
-    --executor-memory "${EXECUTOR_MEMORY}G" \
-    --conf spark.cores.max=$TOTAL_CORES \
-    --conf spark.task.cpus=32 \
-    --conf spark.sql.files.maxPartitionBytes=1073741824 \
-    --conf spark.sql.shuffle.partitions=600 \
-    --conf spark.driver.maxResultSize=2G \
-    --conf spark.locality.wait=0s \
-    --conf spark.network.timeout=1800s \
-    --conf spark.task.resource.gpu.amount=1 \
-    --conf spark.executor.resource.gpu.amount=1 \
-    --conf spark.plugins=com.nvidia.spark.SQLPlugin \
-    --conf spark.rapids.sql.concurrentGpuTasks=1 \
-    --conf spark.rapids.sql.reader.batchSizeRows=4000000 \
-    --conf spark.rapids.memory.pinnedPool.size=16g \
-    --conf spark.rapids.sql.explain=ALL \
-    --conf spark.sql.autoBroadcastJoinThreshold=1GB \
-    --conf spark.rapids.sql.incompatibleOps.enabled=true \
-    --conf spark.driver.maxResultSize=2G \
-    --conf spark.executor.extraJavaOptions="-Dcom.nvidia.cudf.prefer-pinned=true\ -Djava.io.tmpdir=$SPARK_LOCAL_DIRS" \
-    spark_data_utils.py --mode transform \
-    --input_folder $temp_validation \
-    --days 23-23 \
-    --output_folder $OUTPUT_PATH/validation \
-    --output_ordering input \
-    --model_folder $OUTPUT_PATH/models \
-    --write_mode overwrite --low_mem 2>&1 | tee submit_validation_log.txt
+    --write_mode overwrite --low_mem 2>&1 | tee submit_day${i}_log.txt
+done

-rm -r $temp_test $temp_validation
 stop-master.sh
 stop-slave.sh
