pipeline:
  name: ArXiv dataset Test Pipeline
  description: Example pipeline
  version: 1.0.0
model:
  id: DistilBertNet
  config:
    num_classes: 172
training:
  gpus: 1
  device: "cuda:0"
  dataloader_workers: 2
  use_previous_model: True
  initial_model: random
  initial_pass:
    activated: False
  batch_size: 64
  optimizers:
    - name: "default"
      algorithm: "SGD"
      source: "PyTorch"
      param_groups:
        - module: "model"
          config:
            lr: 0.1
            momentum: 0.001
  optimization_criterion:
    name: "CrossEntropyLoss"
  checkpointing:
    activated: False
  selection_strategy:
    name: NewDataStrategy
    maximum_keys_in_memory: 1000
    config:
      limit: -1
      reset_after_trigger: True
data:
  dataset_id: arxiv
  bytes_parser_function: |
    import numpy as np
    import io
    def bytes_parser_function(data: bytes) -> np.ndarray:
      return np.load(io.BytesIO(data))

trigger:
  id: TimeTrigger
  trigger_config:
    trigger_every: "1d"