{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from analytics.app.data.load import list_pipelines, load_pipeline_logs\n",
    "from analytics.app.data.transform import (\n",
    "    df_aggregate_eval_metric,\n",
    "    dfs_models_and_evals,\n",
    "    pipeline_leaf_times_df,\n",
    ")\n",
    "from analytics.plotting.common.save import save_plot\n",
    "from analytics.plotting.common.tradeoff_scatterplot import plot_tradeoff_scatter\n",
    "from modyn.supervisor.internal.grpc.enums import PipelineStage\n",
    "from modyn.supervisor.internal.pipeline_executor.models import PipelineLogs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines_dirs = [\n",
    "    Path(\"/Users/robinholzinger/robin/dev/eth/modyn-robinholzi-data/data/triggering/arxiv/10_baselines_time\"),\n",
    "    Path(\"/Users/robinholzinger/robin/dev/eth/modyn-robinholzi-data/data/triggering/arxiv/11_baselines_amount\"),\n",
    "    Path(\n",
    "        \"/Users/robinholzinger/robin/dev/eth/modyn-robinholzi-data/data/triggering/arxiv/21_datadrift_dynamic\"\n",
    "    ),  # TODO\n",
    "    Path(\"/Users/robinholzinger/robin/dev/eth/modyn-robinholzi-data/data/triggering/arxiv/30_performance\"),\n",
    "]\n",
    "\n",
    "pipeline_logs: dict[int, PipelineLogs] = {}\n",
    "pipelines: dict[int, tuple[str, Path]] = {}\n",
    "\n",
    "for dir in pipelines_dirs:\n",
    "    print(\"Reading\", dir)\n",
    "    dir_pipelines = list_pipelines(dir)\n",
    "    print(dir_pipelines)\n",
    "    pipelines.update(dir_pipelines)\n",
    "\n",
    "    max_pipeline_id = max(dir_pipelines.keys())\n",
    "    print(pipelines)\n",
    "    pipeline_logs.update({p_id: load_pipeline_logs(p_id, dir) for (p_id, (_, p_path)) in dir_pipelines.items()})\n",
    "    assert dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't do anything unless include_composite_model = True\n",
    "composite_model_variant = \"currently_active_model\"\n",
    "\n",
    "patch_yearbook = True\n",
    "dataset_id = \"arxiv_kaggle_test\"\n",
    "eval_handler = \"periodic-current\"\n",
    "metric = \"Accuracy\"\n",
    "include_composite_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {p_id: (pname, p_path) for p_id, (pname, p_path) in pipelines.items()}\n",
    "\n",
    "pipeline_ids = pipelines.keys()\n",
    "pipeline_ids = [\n",
    "    y\n",
    "    for y, _ in [\n",
    "        (263, \"timetrigger_5y\"),\n",
    "        (265, \"timetrigger_10y\"),\n",
    "        # (267, 'timetrigger_26w'),\n",
    "        (269, \"timetrigger_2y\"),\n",
    "        (272, \"timetrigger_1y\"),\n",
    "        # (264, 'dataamount_1000000'),\n",
    "        (266, \"dataamount_50000\"),\n",
    "        # (268, 'dataamount_500000'),\n",
    "        (270, \"dataamount_25000\"),\n",
    "        (271, \"dataamount_100000\"),\n",
    "        (782, \"drifttrigger_mmd-quant-0.05-20_int20000_win1y\"),\n",
    "        (783, \"drifttrigger_mmd-rollavg-0.5-20_int20000_win1y\"),\n",
    "        (784, \"drifttrigger_mmd-rollavg-5.0-20_int20000_win1y\"),\n",
    "        (785, \"drifttrigger_mmd-quant-0.15-20_int20000_win1y\"),\n",
    "        (786, \"drifttrigger_mmd-rollavg-0.2-20_int20000_win1y\"),\n",
    "        (787, \"drifttrigger_mmd-quant-0.1-20_int20000_win1y\"),\n",
    "        (788, \"drifttrigger_mmd-rollavg-1.0-20_int20000_win1y\"),\n",
    "        (789, \"drifttrigger_mmd-quant-0.3-20_int20000_win1y\"),\n",
    "        (790, \"drifttrigger_mmd-rollavg-2.0-20_int20000_win1y\"),\n",
    "        (674, \"performancetrigger_static-0.45-int20000\"),\n",
    "        (675, \"performancetrigger_dynamic-quant-0.05-20-int20000\"),\n",
    "        (676, \"performancetrigger_dynamic-rollavg-0.3-20-int20000\"),\n",
    "        (677, \"performancetrigger_num_misclass-100000-exp-0.6-red-False--int20000\"),\n",
    "        (678, \"performancetrigger_dynamic-rollavg-0.2-20-int20000\"),\n",
    "        (679, \"performancetrigger_dynamic-rollavg-0.1-20-int20000\"),\n",
    "        (680, \"performancetrigger_static-0.5-int20000\"),\n",
    "        (681, \"performancetrigger_dynamic-quant-0.15-20-int20000\"),\n",
    "        (682, \"performancetrigger_num_misclass-50000-exp-0.6-red-False--int20000\"),\n",
    "        (723, \"performancetrigger_num_misclass-30000-exp-0.6-red-False--int20000\"),\n",
    "        (756, \"performancetrigger_num_misclass-15000-exp-0.6-red-False--int20000\"),\n",
    "        (762, \"performancetrigger_num_misclass-10000-exp-0.6-red-False--int20000\"),\n",
    "    ]\n",
    "]\n",
    "\n",
    "[(p_id, pname) for p_id, (pname, _) in pipelines.items() if p_id in pipeline_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_eval_single: list[pd.DataFrame] = []\n",
    "df_leaf_list: list[pd.DataFrame] = []\n",
    "\n",
    "for pipeline_id in pipeline_ids:\n",
    "    logs = pipeline_logs[pipeline_id]\n",
    "    df_leaf_single = pipeline_leaf_times_df(logs, use_traintime_patch_at_trainer=True, pipeline_id=pipeline_id)\n",
    "    df_leaf_single[\"pipeline_id\"] = pipeline_id\n",
    "    df_leaf_list.append(df_leaf_single)\n",
    "\n",
    "    _, _, df_eval_single = dfs_models_and_evals(\n",
    "        pipeline_logs[pipeline_id], df_leaf_single[\"sample_time\"].max(), pipelines[pipeline_id][0]\n",
    "    )\n",
    "    df_eval_single[\"pipeline_id\"] = pipeline_id\n",
    "    list_df_eval_single.append(df_eval_single)\n",
    "\n",
    "df_adjusted = pd.concat(list_df_eval_single)\n",
    "df_adjusted\n",
    "\n",
    "df_leaf = pd.concat(df_leaf_list)\n",
    "df_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_leaf[\"id\"].unique())\n",
    "assert set(df_leaf[\"id\"].unique()) == {\n",
    "    \"TRAIN\",\n",
    "    \"INIT_CLUSTER_CONNECTION\",\n",
    "    \"EVALUATE_TRIGGER_POLICY\",\n",
    "    \"INFORM_SELECTOR_REMAINING_DATA\",\n",
    "    \"INFORM_SELECTOR_ABOUT_TRIGGER\",\n",
    "    \"TRAINING_COMPLETED\",\n",
    "    \"STORE_TRAINED_MODEL\",\n",
    "    \"EVALUATE\",\n",
    "    \"DONE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = df_adjusted[\n",
    "    (df_adjusted[\"dataset_id\"] == dataset_id)\n",
    "    & (df_adjusted[\"eval_handler\"] == eval_handler)\n",
    "    & (df_adjusted[\"metric\"] == metric)\n",
    "]\n",
    "\n",
    "# in percent (0-100)\n",
    "df_adjusted[\"value\"] = df_adjusted[\"value\"] * 100\n",
    "df_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = df_adjusted.sort_values(by=[\"interval_center\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to composite models\n",
    "df_adjusted = df_adjusted[df_adjusted[composite_model_variant]]\n",
    "df_adjusted[composite_model_variant].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce evaluation interval to interval where all policies have evaluations\n",
    "min_active_eval_center_per_pipeline = (\n",
    "    df_adjusted[df_adjusted[composite_model_variant]].groupby(\"pipeline_ref\")[\"interval_center\"].min()\n",
    ")\n",
    "\n",
    "maximum_min = pd.to_datetime(min_active_eval_center_per_pipeline).max()\n",
    "print(maximum_min, min_active_eval_center_per_pipeline)\n",
    "\n",
    "assert maximum_min < pd.to_datetime(\"2006-01-01\")\n",
    "\n",
    "df_adjusted = df_adjusted[pd.to_datetime(df_adjusted[\"interval_center\"]) >= maximum_min]\n",
    "df_adjusted[\"interval_center\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted[\"interval_center\"] = df_adjusted[\"interval_center\"].astype(str).str.split(\"-\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics to a scalar value per pipeline\n",
    "mean_accuracies = df_aggregate_eval_metric(\n",
    "    df_adjusted,\n",
    "    group_by=[\"pipeline_id\", \"pipeline_ref\", \"metric\"],\n",
    "    in_col=\"value\",\n",
    "    out_col=\"metric_value\",\n",
    "    aggregate_func=\"mean\",\n",
    ")\n",
    "mean_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triggers = df_leaf[df_leaf[\"id\"] == PipelineStage.TRAIN.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triggers = df_leaf[df_leaf[\"id\"] == PipelineStage.TRAIN.name]\n",
    "df_triggers = df_triggers[df_triggers[\"sample_time\"] > maximum_min]\n",
    "df_triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of trigger per pipeline that are after maximum_min\n",
    "\n",
    "# before the cutoff there was one trigger (equivalent to start of our reduced dataset): +1\n",
    "num_triggers = df_triggers.groupby(\"pipeline_id\").aggregate(count=(\"id\", \"count\"), sum_duration=(\"duration\", \"sum\"))\n",
    "num_triggers[\"count\"] += 1\n",
    "num_triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = num_triggers.merge(mean_accuracies, on=\"pipeline_id\", how=\"inner\")\n",
    "assert num_triggers.shape[0] == merged.shape[0]\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_type(x: str):\n",
    "    if \"year\" in x:\n",
    "        return \"time\"\n",
    "    elif \"samples\" in x:\n",
    "        return \"amount\"\n",
    "    elif \"d\" in x:\n",
    "        return \"drift\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "merged[\"type\"] = merged[\"pipeline_ref\"].apply(lambda x: create_type(x))\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = merged.copy()\n",
    "\n",
    "# renamed = merged[\n",
    "#     merged[\"pipeline_id\"].isin(\n",
    "#         [\n",
    "#             # # static thresholds\n",
    "#             # 113,  # 0.03\n",
    "#             # 112,  # 0.05\n",
    "#             # 107,  # 0.07\n",
    "#             # 109,  # 0.09\n",
    "#             # 85,  # 0.12\n",
    "#             # # dyn quantile\n",
    "#             # 353,  # % 0.05\n",
    "#             # 345,  # % 0.10\n",
    "#             # 357,  # % 0.15\n",
    "#             # # dyn roll. avg\n",
    "#             # 372,  # Δ 2.0\n",
    "#             # 370,  # Δ 1.0\n",
    "#             # 369,  # Δ 0.5\n",
    "#             # 363,  # Δ 0.05\n",
    "#         ]\n",
    "#     )\n",
    "# ].copy()\n",
    "renamed[\"Trigger SubType\"] = renamed[\"pipeline_ref\"].apply(\n",
    "    lambda x: (\n",
    "        \"DataAmount\"\n",
    "        if \"dataamount\" in x\n",
    "        else (\n",
    "            \"Time\"\n",
    "            if \"time\" in x\n",
    "            else (\n",
    "                (\n",
    "                    \"Static\"\n",
    "                    if \"_mmd-0\" in x\n",
    "                    else (\"Quantile\" if \"quant\" in x else (\"Rolling Avg.\" if \"roll\" in x else (\"unknown\")))\n",
    "                )\n",
    "                if \"drift\" in x\n",
    "                else (\n",
    "                    (\n",
    "                        \"Static\"\n",
    "                        if \"static\" in x\n",
    "                        else (\n",
    "                            \"Quantile\"\n",
    "                            if \"quant\" in x\n",
    "                            else (\n",
    "                                \"Rolling Avg.\"\n",
    "                                if \"roll\" in x\n",
    "                                else (\"AvoidableMisclass\" if \"num_misclass\" in x else (\"unknown\"))\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                    if \"performancetrigger\" in x\n",
    "                    else (\n",
    "                        \"DataIncorporationLatency\"\n",
    "                        if \"data_inc\" in x\n",
    "                        else (\"AvoidableMisclass\" if \"avoidable\" in x else (\"unknown\"))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "renamed[\"Trigger Type\"] = renamed[\"pipeline_ref\"].apply(\n",
    "    lambda x: (\n",
    "        \"Simple\"\n",
    "        if \"dataamount\" in x\n",
    "        else (\n",
    "            \"Simple\"\n",
    "            if \"time\" in x\n",
    "            else (\n",
    "                \"DataDrift\"\n",
    "                if \"drift\" in x\n",
    "                else (\"Performance\" if \"performancetrigger\" in x else (\"Cost\" if \"costtrigger\" in x else (\"unknown\")))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# assert no unknowns and DataIncorporationLatency\n",
    "assert not renamed[\"Trigger Type\"].str.contains(\"unknown\").any()\n",
    "assert not renamed[\"Trigger SubType\"].str.contains(\"unknown\").any()\n",
    "assert not renamed[\"Trigger SubType\"].str.contains(\"DataIncorporationLatency\").any()\n",
    "\n",
    "# assert no cost triggers\n",
    "assert not renamed[\"Trigger Type\"].str.contains(\"Cost\").any()\n",
    "\n",
    "renamed[\"Trigger Type\"] = pd.Categorical(\n",
    "    renamed[\"Trigger Type\"], categories=[\"Simple\", \"DataDrift\", \"Performance\"], ordered=True\n",
    ")\n",
    "\n",
    "renamed[\"Trigger SubType\"] = pd.Categorical(\n",
    "    renamed[\"Trigger SubType\"],\n",
    "    categories=[\"DataAmount\", \"Time\", \"Static\", \"Quantile\", \"Rolling Avg.\", \"AvoidableMisclass\"],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "renamed = renamed.sort_values(by=[\"Trigger Type\", \"Trigger SubType\", \"pipeline_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tradeoff_scatter(\n",
    "    renamed,\n",
    "    x=\"count\",\n",
    "    y=\"metric_value\",\n",
    "    hue=\"Trigger Type\",\n",
    "    style=\"Trigger SubType\",\n",
    "    x_label=\"Number of Triggers\",\n",
    "    y_label=\"Mean Accuracy %\",\n",
    "    height_factor=0.8,\n",
    "    width_factor=0.8,\n",
    "    manual_legend_title=False,\n",
    "    legend_ncol=2,\n",
    ")\n",
    "\n",
    "save_plot(fig, \"_all_tradeoff_arxiv_triggers_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_minutes = renamed.copy()\n",
    "in_minutes[\"sum_duration\"] = in_minutes[\"sum_duration\"] / 60\n",
    "\n",
    "fig = plot_tradeoff_scatter(\n",
    "    in_minutes,\n",
    "    x=\"sum_duration\",\n",
    "    y=\"metric_value\",\n",
    "    hue=\"Trigger Type\",\n",
    "    style=\"Trigger SubType\",\n",
    "    x_label=\"Total Cost (Minutes)\",\n",
    "    y_label=\"Mean Accuracy %\",\n",
    "    height_factor=0.7,\n",
    "    width_factor=0.8,\n",
    "    manual_legend_title=False,\n",
    "    legend_ncol=2,\n",
    ")\n",
    "\n",
    "# save_plot(fig, \"tradeoff_drift_yearbook_cost_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tradeoff_scatter(\n",
    "    renamed,\n",
    "    x=\"count\",\n",
    "    y=\"sum_duration\",\n",
    "    hue=\"Trigger Type\",\n",
    "    style=\"Trigger SubType\",\n",
    "    x_label=\"Number of Triggers\",\n",
    "    y_label=\"Total Cost (seconds)\",\n",
    "    height_factor=1.5,\n",
    "    width_factor=1.8,\n",
    ")\n",
    "\n",
    "# save_plot(fig, \"tradeoff_drift_yearbook_triggers_cost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
