{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from analytics.app.data.load import list_pipelines, load_pipeline_logs\n",
    "from analytics.app.data.transform import (\n",
    "    dfs_models_and_evals,\n",
    "    patch_yearbook_time,\n",
    "    pipeline_leaf_times_df,\n",
    ")\n",
    "from analytics.plotting.common.save import save_plot\n",
    "from modyn.supervisor.internal.pipeline_executor.models import PipelineLogs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines_dirs = [\n",
    "    Path(\"/Users/robinholzinger/robin/dev/eth/modyn-robinholzi-data/data/triggering/yearbook/20_datadrift_static\"),\n",
    "    Path(\"/Users/robinholzinger/robin/dev/eth/modyn-robinholzi-data/data/triggering/yearbook/21_datadrift_dynamic\"),\n",
    "]\n",
    "\n",
    "pipeline_logs: dict[int, PipelineLogs] = {}\n",
    "pipelines: dict[int, tuple[str, Path]] = {}\n",
    "\n",
    "for dir in pipelines_dirs:\n",
    "    dir_pipelines = list_pipelines(dir)\n",
    "    pipelines.update(dir_pipelines)\n",
    "    max_pipeline_id = max(dir_pipelines.keys())\n",
    "    print(pipelines)\n",
    "    pipeline_logs.update({p_id: load_pipeline_logs(p_id, dir) for (p_id, (_, p_path)) in dir_pipelines.items()})\n",
    "    assert dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ids = list(pipelines.keys())\n",
    "\n",
    "# doesn't do anything unless include_composite_model = True\n",
    "composite_model_variant = \"currently_active_model\"\n",
    "\n",
    "patch_yearbook = True\n",
    "dataset_id = \"yearbook_test\"\n",
    "eval_handler = \"periodic-delta+-1y\"\n",
    "metric = \"Accuracy\"\n",
    "include_composite_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_eval_single: list[pd.DataFrame] = []\n",
    "df_logs_models_list: list[pd.DataFrame] = []\n",
    "\n",
    "for pipeline_id in pipeline_ids:\n",
    "    logs = pipeline_logs[pipeline_id]\n",
    "    df_leaf_single = pipeline_leaf_times_df(logs, use_traintime_patch_at_trainer=False, pipeline_id=pipeline_id)\n",
    "    df_logs_models_single, _, df_eval_single = dfs_models_and_evals(\n",
    "        pipeline_logs[pipeline_id], df_leaf_single[\"sample_time\"].max(), pipelines[pipeline_id][0]\n",
    "    )\n",
    "    df_eval_single[\"pipeline_id\"] = pipeline_id\n",
    "    df_logs_models_single[\"pipeline_id\"] = pipeline_id\n",
    "    list_df_eval_single.append(df_eval_single)\n",
    "    df_logs_models_list.append(df_logs_models_single)\n",
    "\n",
    "df_adjusted = pd.concat(list_df_eval_single)\n",
    "df_adjusted\n",
    "\n",
    "df_logs_models = pd.concat(df_logs_models_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = df_adjusted[\n",
    "    (df_adjusted[\"dataset_id\"] == dataset_id)\n",
    "    & (df_adjusted[\"eval_handler\"] == eval_handler)\n",
    "    & (df_adjusted[\"metric\"] == metric)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if patch_yearbook:\n",
    "    for column in [\"interval_start\", \"interval_center\", \"interval_end\"]:\n",
    "        patch_yearbook_time(df_adjusted, column)\n",
    "    for column in [\"train_start\", \"train_end\", \"real_train_end\", \"usage_start\", \"usage_end\"]:\n",
    "        patch_yearbook_time(df_logs_models, column)\n",
    "\n",
    "    # correction for -1 second in timestamp format before patching\n",
    "    df_logs_models[\"usage_end\"] = (\n",
    "        df_logs_models[\"usage_end\"].dt.to_period(\"M\") + 1\n",
    "    ).dt.to_timestamp()  # december (because of -1 second in timestamp format) -> start of year\n",
    "\n",
    "df_logs_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = df_adjusted.sort_values(by=[\"interval_center\"])\n",
    "len(df_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to composite models\n",
    "df_adjusted = df_adjusted[df_adjusted[composite_model_variant]]\n",
    "df_adjusted[composite_model_variant].unique()\n",
    "len(df_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted[\"interval_center\"] = df_adjusted[\"interval_center\"].astype(str).str.split(\"-\").str[0]\n",
    "\n",
    "df_train_end_years_per_model = df_logs_models[[\"pipeline_id\", \"model_idx\", \"real_train_end\"]]\n",
    "df_train_end_years_per_model[\"real_train_end\"] = df_train_end_years_per_model[\"real_train_end\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted.groupby([\"pipeline_id\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted[\"value\"] = df_adjusted[\"value\"] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pids = [117, 107, 95]\n",
    "\n",
    "df_merged = df_adjusted.merge(df_train_end_years_per_model, on=[\"pipeline_id\", \"model_idx\"], how=\"left\")\n",
    "# build heatmap matrix dataframe:\n",
    "df_merged[\"pipeline_id\"] = df_merged[\"pipeline_id\"].astype(int)\n",
    "df_merged = df_merged[df_merged[\"pipeline_id\"].isin(_pids)]\n",
    "heatmap_data = df_merged.pivot(index=[\"pipeline_id\"], columns=\"interval_center\", values=\"value\")\n",
    "\n",
    "heatmap_data.index.min(), heatmap_data.index.max()\n",
    "heatmap_data\n",
    "\n",
    "# sort index by pipeline_refs\n",
    "heatmap_data = heatmap_data.reindex(_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytics.plotting.common.heatmap import build_heatmap\n",
    "\n",
    "pipelines_refs = {\n",
    "    117: \"10y\",\n",
    "    107: \"4y\",\n",
    "    95: \"1y\",\n",
    "}\n",
    "\n",
    "fig = build_heatmap(\n",
    "    heatmap_data,\n",
    "    reverse_col=True,\n",
    "    x_ticks=[1950, 1975, 2000],\n",
    "    y_custom_ticks=[(i + 0.5, pipelines_refs[y]) for i, y in enumerate(heatmap_data.index)],\n",
    "    y_label=\"Pipeline with\\nWindow Size\",\n",
    "    x_label=\"Evaluation Year\",\n",
    "    title_label=\"Yearbook Composite Models: Drift Window Sizes (MMD=0.07)\",\n",
    "    color_label=\"Accuracy %\",\n",
    "    width_factor=1,\n",
    "    height_factor=0.38,\n",
    "    # grid_alpha=0.4,\n",
    "    grid_alpha=0.0,\n",
    "    # disable_horizontal_grid=True,\n",
    "    # cbar=False,\n",
    "    triggers={\n",
    "        i: df_logs_models[df_logs_models[\"pipeline_id\"] == p_id][\n",
    "            [\"train_start\", \"train_end\", \"usage_start\", \"usage_end\"]\n",
    "        ]\n",
    "        for i, p_id in enumerate(heatmap_data.index)\n",
    "    },\n",
    ")\n",
    "save_plot(fig, \"yb_trigger_heatmap_drift_multi_static_window_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pids = list(reversed([113, 112, 107, 109, 85]))\n",
    "\n",
    "df_merged = df_adjusted.merge(df_train_end_years_per_model, on=[\"pipeline_id\", \"model_idx\"], how=\"left\")\n",
    "# build heatmap matrix dataframe:\n",
    "df_merged[\"pipeline_id\"] = df_merged[\"pipeline_id\"].astype(int)\n",
    "df_merged = df_merged[df_merged[\"pipeline_id\"].isin(_pids)]\n",
    "heatmap_data = df_merged.pivot(index=[\"pipeline_id\"], columns=\"interval_center\", values=\"value\")\n",
    "\n",
    "heatmap_data.index.min(), heatmap_data.index.max()\n",
    "\n",
    "# sort index by pipeline_refs\n",
    "heatmap_data = heatmap_data.reindex(_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytics.plotting.common.heatmap import build_heatmap\n",
    "from analytics.plotting.common.save import save_plot\n",
    "\n",
    "pipelines_refs = {\n",
    "    113: \"0.03\",\n",
    "    112: \"0.05\",\n",
    "    107: \"0.07\",\n",
    "    109: \"0.09\",\n",
    "    85: \"0.12\",\n",
    "}\n",
    "\n",
    "fig = build_heatmap(\n",
    "    heatmap_data,\n",
    "    reverse_col=True,\n",
    "    x_ticks=[1950, 1975, 2000],\n",
    "    y_custom_ticks=[(i + 0.5, pipelines_refs[y]) for i, y in enumerate(heatmap_data.index)],\n",
    "    y_label=\"MMD Threshold\",\n",
    "    x_label=\"Evaluation Year\",\n",
    "    title_label=\"Yearbook Composite Models: Static Drift Thresholds\",\n",
    "    color_label=\"Accuracy %\",\n",
    "    width_factor=1,\n",
    "    height_factor=0.45,\n",
    "    # grid_alpha=0.4,\n",
    "    grid_alpha=0.0,\n",
    "    # disable_horizontal_grid=True,\n",
    "    # cbar=False,\n",
    "    triggers={\n",
    "        i: df_logs_models[df_logs_models[\"pipeline_id\"] == p_id][\n",
    "            [\"train_start\", \"train_end\", \"usage_start\", \"usage_end\"]\n",
    "        ]\n",
    "        for i, p_id in enumerate(heatmap_data.index)\n",
    "    },\n",
    ")\n",
    "save_plot(fig, \"yb_trigger_heatmap_drift_multi_static_threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Quantile / Roll Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pids = list([372, 370, 369, 363]) + list([353, 357])  # 345\n",
    "\n",
    "df_merged = df_adjusted.merge(df_train_end_years_per_model, on=[\"pipeline_id\", \"model_idx\"], how=\"left\")\n",
    "# build heatmap matrix dataframe:\n",
    "df_merged[\"pipeline_id\"] = df_merged[\"pipeline_id\"].astype(int)\n",
    "df_merged = df_merged[df_merged[\"pipeline_id\"].isin(_pids)]\n",
    "heatmap_data = df_merged.pivot(index=[\"pipeline_id\"], columns=\"interval_center\", values=\"value\")\n",
    "\n",
    "heatmap_data.index.min(), heatmap_data.index.max()\n",
    "\n",
    "# sort index by pipeline_refs\n",
    "heatmap_data = heatmap_data.reindex(_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytics.plotting.common.heatmap import build_heatmap\n",
    "from analytics.plotting.common.save import save_plot\n",
    "\n",
    "pipelines_refs = {\n",
    "    # dyn quantile\n",
    "    353: \"% 0.05\",\n",
    "    # 345: \"% 0.10\",\n",
    "    357: \"% 0.15\",\n",
    "    # dyn roll. avg\n",
    "    372: \"Δ 2.0\",\n",
    "    370: \"Δ 1.0\",\n",
    "    369: \"Δ 0.5\",\n",
    "    363: \"Δ 0.05\",\n",
    "}\n",
    "\n",
    "fig = build_heatmap(\n",
    "    heatmap_data,\n",
    "    reverse_col=True,\n",
    "    x_ticks=[1950, 1975, 2000],\n",
    "    y_custom_ticks=[(i + 0.5, pipelines_refs[y]) for i, y in enumerate(heatmap_data.index)],\n",
    "    y_label=\"Criterion\",\n",
    "    x_label=\"Evaluation Year\",\n",
    "    title_label=\"Yearbook Composite Models: Dynamic Drift Thresholds\",\n",
    "    color_label=\"Accuracy %\",\n",
    "    width_factor=1,\n",
    "    height_factor=0.47,\n",
    "    # grid_alpha=0.4,\n",
    "    grid_alpha=0.0,\n",
    "    # disable_horizontal_grid=True,\n",
    "    # cbar=False,\n",
    "    triggers={\n",
    "        i: df_logs_models[df_logs_models[\"pipeline_id\"] == p_id][\n",
    "            [\"train_start\", \"train_end\", \"usage_start\", \"usage_end\"]\n",
    "        ]\n",
    "        for i, p_id in enumerate(heatmap_data.index)\n",
    "    },\n",
    ")\n",
    "save_plot(fig, \"yb_trigger_heatmap_drift_multi_dynamic_thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
