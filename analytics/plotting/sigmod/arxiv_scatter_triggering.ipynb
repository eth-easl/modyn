{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from analytics.app.data.load import list_pipelines\n",
    "from analytics.app.data.transform import (\n",
    "    df_aggregate_eval_metric,\n",
    "    dfs_models_and_evals,\n",
    "    logs_dataframe,\n",
    ")\n",
    "from analytics.plotting.common.common import init_plot\n",
    "from modyn.supervisor.internal.grpc.enums import PipelineStage\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "\n",
    "pipelines_dir = Path(\"/Users/mboether/phd/dynamic-data/sigmod-data/arxiv/revision/final_arxiv_revision_logs_agg\")\n",
    "output_dir = Path(\"/Users/mboether/phd/dynamic-data/dynamic_datasets_dsl/analytics/plotting/sigmod\")\n",
    "assert pipelines_dir.exists()\n",
    "assert output_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = list_pipelines(pipelines_dir)\n",
    "max_pipeline_id = max(pipelines.keys())\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytics.app.data.load import load_pipeline_logs\n",
    "\n",
    "pipeline_logs = {p_id: load_pipeline_logs(p_id, pipelines_dir) for (p_id, (_, p_path)) in pipelines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode:\n",
    "# single pipeline\n",
    "pipeline_ids = (\n",
    "    [p_id for p_id, (p, _) in pipelines.items() if \"timetrigger\" in p and (\"_1y\" in p or \"_3y\" in p or \"_5y\" in p)]\n",
    "    + [p_id for p_id, (p, _) in pipelines.items() if \"amount\" in p and (\"30000\" in p or \"50000\" in p)]\n",
    "    + [\n",
    "        # drift\n",
    "        p_id\n",
    "        for p_id, (p, _) in pipelines.items()\n",
    "        if p\n",
    "        in {\n",
    "            \"kaggle_arxiv_mmdalibi_5000_0.005_1y\",\n",
    "            \"kaggle_arxiv_mmdalibi_5000_0.01_1y\",\n",
    "            \"kaggle_arxiv_mmdalibi_5000_0.02_1y\",\n",
    "            \"kaggle_arxiv_mmdalibi_dyn_5000_15_qt_0.05_1y\",\n",
    "        }\n",
    "    ]\n",
    "    + [\n",
    "        # perf\n",
    "        p_id\n",
    "        for p_id, (p, _) in pipelines.items()\n",
    "        if p\n",
    "        in {\n",
    "            \"kaggle_arxiv_perf_5000_0.7\",\n",
    "            \"kaggle_arxiv_perf_5000_0.75\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "composite_model_variant = \"currently_active_model\"  # currently_trained_model\n",
    "patch_yearbook = False\n",
    "dataset_id = \"arxiv_kaggle_test\"\n",
    "eval_handler = \"exactmatrix\"\n",
    "metric = \"Top-2-Accuracy\"\n",
    "include_composite_model = False\n",
    "\n",
    "\n",
    "def pipeline_name_mapper(name: str) -> str:\n",
    "    name = name.replace(\"kaggle_arxiv_\", \"\")\n",
    "\n",
    "    if \"amounttrigger\" in name:\n",
    "        name = name.replace(\"amounttrigger_\", \"\")\n",
    "        name = name + \" samples\"\n",
    "    elif \"timetrigger\" in name:\n",
    "        name = name.replace(\"timetrigger_\", \"\")\n",
    "        name = name[:-1] + (\" years\" if not name.endswith(\"1y\") else \" year\")\n",
    "    elif \"perf\" in name:\n",
    "        name = name.replace(\"perf_\", \"\")\n",
    "        name = \"< \" + name.replace(\"5000_\", \"\")\n",
    "    elif \"dyn\" in name:\n",
    "        name = \"AutoDrift\"\n",
    "    else:\n",
    "        name = name.replace(\"mmdalibi_\", \"\")\n",
    "        name = name.replace(\"_\", \"/\")\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "pipelines = {p_id: (pipeline_name_mapper(pname), p_path) for p_id, (pname, p_path) in pipelines.items()}\n",
    "\n",
    "unified_pids = []\n",
    "names = set()\n",
    "for p_id, (pname, _) in pipelines.items():\n",
    "    if p_id in pipeline_ids and pname not in names:\n",
    "        unified_pids.append(p_id)\n",
    "        names.add(pname)\n",
    "pipeline_ids = unified_pids\n",
    "\n",
    "\n",
    "[(p_id, pname) for p_id, (pname, _) in pipelines.items() if p_id in pipeline_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_eval_single: list[pd.DataFrame] = []\n",
    "list_df_all: list[pd.DataFrame] = []\n",
    "\n",
    "for pipeline_id in pipeline_ids:\n",
    "    df_all = logs_dataframe(pipeline_logs[pipeline_id], pipelines[pipeline_id][0])\n",
    "    list_df_all.append(df_all)\n",
    "\n",
    "    _, _, df_eval_single = dfs_models_and_evals(\n",
    "        pipeline_logs[pipeline_id], df_all[\"sample_time\"].max(), pipelines[pipeline_id][0]\n",
    "    )\n",
    "    list_df_eval_single.append(df_eval_single)\n",
    "\n",
    "df_adjusted = pd.concat(list_df_eval_single)\n",
    "df_adjusted\n",
    "\n",
    "df_all = pd.concat(list_df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = df_adjusted[\n",
    "    (df_adjusted[\"dataset_id\"] == dataset_id)\n",
    "    & (df_adjusted[\"eval_handler\"] == eval_handler)\n",
    "    & (df_adjusted[\"metric\"] == metric)\n",
    "]\n",
    "\n",
    "# in percent (0-100)\n",
    "df_adjusted[\"value\"] = df_adjusted[\"value\"] * 100\n",
    "df_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = df_adjusted.sort_values(by=[\"interval_center\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to composite models\n",
    "df_adjusted = df_adjusted[df_adjusted[composite_model_variant]]\n",
    "df_adjusted[composite_model_variant].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump Data backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce evaluation interval to interval where all policies have evaluations\n",
    "min_active_eval_center_per_pipeline = (\n",
    "    df_adjusted[df_adjusted[composite_model_variant]].groupby(\"pipeline_ref\")[\"interval_center\"].min()\n",
    ")\n",
    "maximum_min = min_active_eval_center_per_pipeline.max()\n",
    "print(maximum_min, min_active_eval_center_per_pipeline)\n",
    "\n",
    "df_adjusted = df_adjusted[df_adjusted[\"interval_center\"] >= maximum_min]\n",
    "df_adjusted[\"interval_center\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted[\"interval_center\"] = df_adjusted[\"interval_center\"].astype(str).str.split(\"-\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics to a scalar value per pipeline\n",
    "mean_accuracies = df_aggregate_eval_metric(\n",
    "    df_adjusted,\n",
    "    group_by=[\"pipeline_ref\", \"metric\"],\n",
    "    in_col=\"value\",\n",
    "    out_col=\"metric_value\",\n",
    "    aggregate_func=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triggers = df_all[df_all[\"id\"] == PipelineStage.HANDLE_SINGLE_TRIGGER.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triggers = df_all[df_all[\"id\"] == PipelineStage.HANDLE_SINGLE_TRIGGER.name]\n",
    "df_triggers = df_triggers[df_triggers[\"sample_time\"] > maximum_min]\n",
    "df_triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of trigger per pipeline that are after maximum_min\n",
    "\n",
    "# before the cutoff there was one trigger (equivalent to start of our reduced dataset): +1\n",
    "num_triggers = df_triggers.groupby(\"pipeline_ref\").aggregate(count=(\"id\", \"count\")) + 1\n",
    "num_triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = num_triggers.merge(mean_accuracies, on=\"pipeline_ref\")\n",
    "assert mean_accuracies.shape[0] == merged.shape[0]\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_type(x: str):\n",
    "    if \"year\" in x:\n",
    "        return \"yearly\"\n",
    "    elif \"samples\" in x:\n",
    "        return \"amount\"\n",
    "    elif \"y\" in x:\n",
    "        return \"drift\"\n",
    "    elif \"<\" in x:\n",
    "        return \"perf\"\n",
    "    elif \"AutoDrift\" in x:\n",
    "        return \"drift\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "merged[\"type\"] = merged[\"pipeline_ref\"].apply(lambda x: create_type(x))\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"RdBu\", 10)\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette2 = sns.color_palette(\"colorblind\", 10)\n",
    "palette2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "init_plot()\n",
    "# sns.set_theme(style=\"ticks\")\n",
    "# plt.rcParams['svg.fonttype'] = 'none'\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "FONTSIZE = 20\n",
    "DOUBLE_FIG_WIDTH = 10\n",
    "DOUBLE_FIG_HEIGHT = 3.5\n",
    "DOUBLE_FIG_SIZE = (DOUBLE_FIG_WIDTH, 1.5 * DOUBLE_FIG_HEIGHT)\n",
    "\n",
    "fig = plt.figure(\n",
    "    edgecolor=\"black\",\n",
    "    frameon=True,\n",
    "    figsize=DOUBLE_FIG_SIZE,\n",
    "    dpi=300,\n",
    ")\n",
    "\n",
    "markers = {\"drift\": \"X\", \"yearly\": \"o\", \"amount\": \"D\", \"perf\": \"*\"}\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    merged,\n",
    "    x=\"count\",\n",
    "    y=\"metric_value\",\n",
    "    hue=\"type\",\n",
    "    palette={\"drift\": palette[-2], \"yearly\": palette2[1], \"amount\": palette[1], \"perf\": palette2[4]},\n",
    "    s=200,\n",
    "    legend=True,\n",
    "    markers=markers,\n",
    "    style=\"type\",  # required for markers\n",
    "    edgecolor=\"none\",\n",
    "    # annotations\n",
    ")\n",
    "ax.set(ylim=(55, 80))\n",
    "ax.set(xlim=(-4, 170))\n",
    "\n",
    "for i in range(merged.shape[0]):\n",
    "    offsets = defaultdict(lambda: (+1.5, -0.25))\n",
    "    offsets.update(\n",
    "        {\n",
    "            # x, y\n",
    "            \"3 years\": (+2, -1),\n",
    "            \"1 year\": (-3.5, +1),\n",
    "            \"5 years\": (+2, -0.15),\n",
    "            \"50000 samples\": (+3, -1),\n",
    "            \"30000 samples\": (-3, +1),\n",
    "            \"5000/0.005/1y\": (-9, -2),\n",
    "            \"5000/0.01/1y\": (+2.5, -0.5),\n",
    "            \"5000/0.02/1y\": (+2.5, -0.5),\n",
    "            \"< 0.75\": (-18, -0.5),\n",
    "            \"< 0.7\": (-3.5, +1),\n",
    "            \"AutoDrift\": (+1.5, -1.5),\n",
    "        }\n",
    "    )\n",
    "    plt.rc(\"text\", usetex=True)\n",
    "\n",
    "    def fix_s(ref: str) -> str:\n",
    "        if ref[0] != \"<\":\n",
    "            return r\"\\textbf{\" + merged[\"pipeline_ref\"][i] + \"}\"\n",
    "\n",
    "        return r\"$\\mathbf{<}$ \\textbf{\" + ref[2:] + \"}\"\n",
    "\n",
    "    plt.text(\n",
    "        x=merged[\"count\"][i] + offsets[merged[\"pipeline_ref\"][i]][0],\n",
    "        y=merged[\"metric_value\"][i] + offsets[merged[\"pipeline_ref\"][i]][1],\n",
    "        s=fix_s(merged[\"pipeline_ref\"][i]),\n",
    "        fontdict=dict(color=\"black\", fontsize=17),\n",
    "    )\n",
    "    plt.rc(\"text\", usetex=False)\n",
    "\n",
    "\n",
    "# Adjust x-axis tick labels\n",
    "plt.xlabel(\"Number of triggers\", labelpad=10)\n",
    "plt.xticks(\n",
    "    ticks=[x for x in range(0, 170 + 1, 20)],\n",
    "    labels=[x for x in range(0, 170 + 1, 20)],\n",
    "    rotation=0,\n",
    "    # ha='right'\n",
    ")\n",
    "\n",
    "# Set y-axis ticks to be equally spaced\n",
    "plt.ylabel(\"Mean Top-2 Accuracy %\", labelpad=15)\n",
    "plt.yticks(\n",
    "    ticks=[x for x in range(56, 80 + 1, 3)],\n",
    "    labels=[x for x in range(56, 80 + 1, 3)],\n",
    "    rotation=0,\n",
    ")\n",
    "\n",
    "\n",
    "label_mapping = {\"drift\": \"Drift\", \"yearly\": \"Time\", \"amount\": \"Amount\", \"perf\": \"Performance\"}\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "latex_labels = [f\"{label_mapping.get(label, label)}  \" for label in labels]\n",
    "\n",
    "legend = ax.legend(\n",
    "    loc=\"lower right\",\n",
    "    ncol=2,\n",
    "    handles=handles,\n",
    "    labels=latex_labels,\n",
    "    labelspacing=0.2,\n",
    "    columnspacing=0.9,\n",
    "    handlelength=1.3,\n",
    ")\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Plot as svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_type in [\"png\", \"svg\"]:\n",
    "    img_path = output_dir / f\"scatter_arxiv.{img_type}\"\n",
    "    fig.savefig(img_path, bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette2[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
