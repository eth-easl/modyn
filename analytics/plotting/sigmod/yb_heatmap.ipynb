{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from analytics.app.data.load import list_pipelines\n",
    "from analytics.app.data.transform import dfs_models_and_evals, logs_dataframe, patch_yearbook_time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "\n",
    "drift_pipeline = True\n",
    "if drift_pipeline:\n",
    "    pipelines_dir = Path(\"/Users/robinholzinger/robin/dev/eth/modyn-sigmod-data/yearbook/triggering/logs_agg\")\n",
    "else:\n",
    "    pipelines_dir = Path(\n",
    "        \"/Users/robinholzinger/robin/dev/eth/modyn-sigmod-data/yearbook/data_selection_50%/logs_agg_patch\"\n",
    "    )\n",
    "output_dir = Path(\"/Users/robinholzinger/robin/dev/eth/modyn-2/.analytics.log/.data/_plots\")\n",
    "assert pipelines_dir.exists()\n",
    "assert output_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = list_pipelines(pipelines_dir)\n",
    "max_pipeline_id = max(pipelines.keys())\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytics.app.data.load import load_pipeline_logs\n",
    "\n",
    "pipeline_logs = {p_id: load_pipeline_logs(p_id, pipelines_dir) for (p_id, (_, p_path)) in pipelines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipeline_logs[5 if not drift_pipeline else 38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode:\n",
    "pipeline_id = 5 if not drift_pipeline else 38\n",
    "\n",
    "# doesn't do anything unless include_composite_model = True\n",
    "composite_model_variant = \"currently_trained_model\" if not drift_pipeline else \"currently_active_model\"\n",
    "\n",
    "patch_yearbook = True\n",
    "dataset_id = \"yearbook_test\"\n",
    "eval_handler = \"slidingmatrix\"\n",
    "metric = \"Accuracy\"\n",
    "include_composite_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log = pipeline_logs[pipeline_id]\n",
    "pipeline_ref = f\"{pipeline_id}\".zfill(len(str(max_pipeline_id))) + f\" - {pipelines[pipeline_id][0]}\"\n",
    "\n",
    "df_all = logs_dataframe(pipeline_log, pipeline_ref)\n",
    "\n",
    "df_logs_models, _, df_eval_single = dfs_models_and_evals(\n",
    "    # subtracting would interfere with yearbook patching\n",
    "    pipeline_log,\n",
    "    df_all[\"sample_time\"].max(),\n",
    "    pipeline_ref,\n",
    ")\n",
    "\n",
    "df_adjusted = df_eval_single\n",
    "\n",
    "df_adjusted = df_adjusted[\n",
    "    (df_adjusted[\"dataset_id\"] == dataset_id)\n",
    "    & (df_adjusted[\"eval_handler\"] == eval_handler)\n",
    "    & (df_adjusted[\"metric\"] == metric)\n",
    "]\n",
    "\n",
    "# in percent (0-100)\n",
    "df_adjusted[\"value\"] = df_adjusted[\"value\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if patch_yearbook:\n",
    "    for column in [\"interval_start\", \"interval_center\", \"interval_end\"]:\n",
    "        patch_yearbook_time(df_adjusted, column)\n",
    "    for column in [\"train_start\", \"train_end\", \"real_train_end\", \"usage_start\", \"usage_end\"]:\n",
    "        patch_yearbook_time(df_logs_models, column)\n",
    "\n",
    "    # correction for -1 second in timestamp format before patching\n",
    "    df_logs_models[\"usage_end\"] = (\n",
    "        df_logs_models[\"usage_end\"].dt.to_period(\"M\") + 1\n",
    "    ).dt.to_timestamp()  # december (because of -1 second in timestamp format) -> start of year\n",
    "\n",
    "df_logs_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = df_adjusted.sort_values(by=[\"interval_center\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add composite model\n",
    "\n",
    "assert df_adjusted[\"pipeline_ref\"].nunique() <= 1\n",
    "# add the pipeline time series which is the performance of different models stitched together dep.\n",
    "# w.r.t which model was active\n",
    "pipeline_composite_model = df_adjusted[df_adjusted[composite_model_variant]]\n",
    "pipeline_composite_model[\"model_idx\"] = 0\n",
    "pipeline_composite_model[\"id_model\"] = 0\n",
    "\n",
    "label_map = {k: f\"{k}\" for k, v in df_adjusted[[\"model_idx\", \"id_model\"]].values}\n",
    "label_map[0] = \"Pipeline composite model\"\n",
    "\n",
    "if include_composite_model:\n",
    "    df_adjusted = pd.concat([pipeline_composite_model, df_adjusted])\n",
    "else:\n",
    "    df_adjusted[\"model_idx\"] = df_adjusted[\"model_idx\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump Data backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted[\"interval_center\"] = df_adjusted[\"interval_center\"].astype(str).str.split(\"-\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_end_years_per_model = df_logs_models[[\"model_idx\", \"real_train_end\"]]\n",
    "df_train_end_years_per_model[\"real_train_end\"] = df_train_end_years_per_model[\"real_train_end\"].dt.year\n",
    "df_train_end_years_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_adjusted.merge(df_train_end_years_per_model, on=\"model_idx\", how=\"left\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build heatmap matrix dataframe:\n",
    "heatmap_data = df_merged.pivot(index=[\"real_train_end\"], columns=\"interval_center\", values=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data.index.min(), heatmap_data.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "from analytics.plotting.common.common import INIT_PLOT\n",
    "\n",
    "INIT_PLOT()\n",
    "# sns.set_theme(style=\"ticks\")\n",
    "plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "\n",
    "FONTSIZE = 20\n",
    "DOUBLE_FIG_WIDTH = 10\n",
    "DOUBLE_FIG_HEIGHT = 3.5\n",
    "DOUBLE_FIG_SIZE = (DOUBLE_FIG_WIDTH, (1.5 if drift_pipeline else 2.2) * DOUBLE_FIG_HEIGHT)\n",
    "\n",
    "fig = plt.figure(\n",
    "    edgecolor=\"black\",\n",
    "    frameon=True,\n",
    "    figsize=DOUBLE_FIG_SIZE,\n",
    "    dpi=300,\n",
    ")\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap=\"RdBu_r\",\n",
    "    linewidths=0.0,\n",
    "    linecolor=\"black\",\n",
    "    cbar=True,\n",
    "    # color bar from 0 to 1\n",
    "    cbar_kws={\n",
    "        \"label\": \"Accuracy %\",\n",
    "        \"ticks\": [0, 25, 50, 75, 100],\n",
    "        \"orientation\": \"vertical\",\n",
    "    },\n",
    ")\n",
    "ax.collections[0].set_rasterized(True)\n",
    "\n",
    "# Adjust x-axis tick labels\n",
    "xticks = [x for x in range(1, len(heatmap_data.columns) + 1)]\n",
    "plt.xlabel(\"Evaluation Year\")\n",
    "plt.xticks(\n",
    "    ticks=[x + 0.5 for x in range(0, 2010 - 1930 + 1, 20)],\n",
    "    labels=[x for x in range(1930, 2010 + 1, 20)],\n",
    "    rotation=0,\n",
    "    # ha='right'\n",
    ")\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Set y-axis ticks to be equally spaced\n",
    "# y_ticks = range(25, 100, 25) if not drift_pipeline else range(1, 9, 2)\n",
    "# # -0.5 instead of +0.5 to make 0-based index 1-based\n",
    "# plt.yticks(ticks=[y-0.5 for y in y_ticks], labels=[y for y in y_ticks], rotation=0)\n",
    "# plt.ylabel(\"Model Index\")\n",
    "\n",
    "if not drift_pipeline:\n",
    "    plt.yticks(\n",
    "        ticks=[x + 0.5 for x in range(0, 2010 - 1930 + 1, 20)],\n",
    "        labels=[x for x in range(1930, 2010 + 1, 20)],\n",
    "        rotation=0,\n",
    "        # ha='right'\n",
    "    )\n",
    "plt.ylabel(\"Trained up to\")\n",
    "\n",
    "# Draft training boxes\n",
    "if drift_pipeline:\n",
    "    for type_, dashed in [(\"train\", False), (\"usage\", False), (\"train\", True)]:\n",
    "        for active_ in df_logs_models.iterrows():\n",
    "            x_start = active_[1][f\"{type_}_start\"].year - 1930\n",
    "            x_end = active_[1][f\"{type_}_end\"].year - 1930\n",
    "            y = active_[1][\"model_idx\"]\n",
    "            rect = plt.Rectangle(\n",
    "                (x_start, y - 1),  # y: 0 based index, model_idx: 1 based index\n",
    "                x_end - x_start,\n",
    "                1,\n",
    "                edgecolor=\"White\" if type_ == \"train\" else \"Black\",\n",
    "                facecolor=\"none\",\n",
    "                linewidth=3,\n",
    "                linestyle=\"dotted\" if dashed else \"solid\",\n",
    "                hatch=\"/\",\n",
    "                joinstyle=\"bevel\",\n",
    "                # capstyle=\"round\",\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_logs_models.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_type in [\"png\", \"svg\"]:\n",
    "    img_path = output_dir / f\"yearbook_heatmap{'_trigger' if drift_pipeline else ''}.{img_type}\"\n",
    "    fig.savefig(img_path, bbox_inches=\"tight\", transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
