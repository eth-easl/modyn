{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "We want to increase the confidence in our pipeline run results by running the same experiment pipelines with different\n",
    "seeds.\n",
    "\n",
    "This yields different evaluation metrics. In consequence, we want to aggregate (e.g. mean, median) the evaluation\n",
    "metrics over runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from modyn.supervisor.internal.pipeline_executor.models import PipelineLogs\n",
    "import pandas as pd\n",
    "from analytics.app.data.transform import logs_dataframe\n",
    "from analytics.app.data.transform import dfs_models_and_evals\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from modyn.supervisor.internal.grpc.enums import PipelineStage\n",
    "from modyn.supervisor.internal.pipeline_executor.models import SingleEvaluationInfo\n",
    "\n",
    "pipeline_files = [\n",
    "    Path(\"/Users/robinholzinger/robin/dev/eth/modyn-2/.analytics.log/.data/pipeline_4/pipeline.log\"),\n",
    "    Path(\"/Users/robinholzinger/robin/dev/eth/modyn-2/.analytics.log/.data/pipeline_5/pipeline.log\"),\n",
    "    Path(\"/Users/robinholzinger/robin/dev/eth/modyn-2/.analytics.log/.data/pipeline_6/pipeline.log\")\n",
    "]\n",
    "\n",
    "aggregated_log_path = Path(\"/Users/robinholzinger/robin/dev/eth/modyn-2/.analytics.log/.data/pipeline_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = [\n",
    "    PipelineLogs.model_validate_json(pipeline_logfile.read_text())\n",
    "    for pipeline_logfile in pipeline_files\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that all pipelines are the same except from the seed\n",
    "\n",
    "candidates = [\n",
    "    deepcopy(log) for log in logs\n",
    "]\n",
    "# set seeds to seed of first pipeline\n",
    "for i, candidate in enumerate(candidates):\n",
    "    candidate.config.pipeline.training.seed = candidates[0].config.pipeline.training.seed\n",
    "\n",
    "assert all(\n",
    "    [candidate.config == candidates[0].config for candidate in candidates]\n",
    "), \"Not all pipelines are the same (ignoring seed)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_logs = [\n",
    "    logs_dataframe(log)\n",
    "    for log in logs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample_time = max([\n",
    "    df[\"sample_time\"].max()\n",
    "    for df in dfs_logs\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_models_evals: list[str, tuple[pd.DataFrame, pd.DataFrame | None, pd.DataFrame | None]] = [\n",
    "    dfs_models_and_evals(log, max_sample_time)\n",
    "    for log in logs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.concat(\n",
    "    [\n",
    "        _df_models for _df_models, _, _ in dfs_models_evals\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_eval_requests = pd.concat(\n",
    "    [\n",
    "        single_df_eval_requests\n",
    "        for _, single_df_eval_requests, _ in dfs_models_evals\n",
    "        if single_df_eval_requests is not None\n",
    "    ]\n",
    ")\n",
    "df_eval_single = pd.concat(\n",
    "    [\n",
    "        _single_eval_df\n",
    "        for _, _, _single_eval_df in dfs_models_evals\n",
    "        if _single_eval_df is not None\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find the primary keys of the models\n",
    "\n",
    "df_eval_single[\n",
    "    (df_eval_single[\"id_model\"] == 2)\n",
    "    & (df_eval_single[\"eval_handler\"] == \"exactmatrix\")\n",
    "    & (df_eval_single[\"dataset_id\"] == \"cglm_hierarchical_min25-test\")\n",
    "    & (df_eval_single[\"interval_start\"] == \"2004-01-01\")\n",
    "    & (df_eval_single[\"interval_end\"] == \"2004-12-31\")\n",
    "    & (df_eval_single[\"metric\"] == \"Accuracy\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_eval_single.groupby(\n",
    "    [\"id_model\", \"eval_handler\", \"dataset_id\", \"interval_start\", \"interval_end\", \"metric\"]\n",
    ")\n",
    "\n",
    "for size in groups.size():\n",
    "    assert size == len(logs), \"Wrong primary key\"\n",
    "\n",
    "aggregated_metrics = groups.agg({\n",
    "    \"value\": \"mean\"\n",
    "}).reset_index()\n",
    "aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modyn.supervisor.internal.grpc.enums import PipelineStage\n",
    "from modyn.supervisor.internal.pipeline_executor.models import SingleEvaluationInfo\n",
    "\n",
    "\n",
    "aggregated_logs = deepcopy(logs[0])\n",
    "for log in aggregated_logs.supervisor_logs.stage_runs:\n",
    "    if log.id == PipelineStage.EVALUATE_SINGLE.name:\n",
    "        assert isinstance(log.info, SingleEvaluationInfo)\n",
    "        if not log.info.results:\n",
    "            continue\n",
    "\n",
    "        eval_req = log.info.eval_request\n",
    "\n",
    "        # find aggregated value\n",
    "        for metric in log.info.results[\"metrics\"]:\n",
    "            lookup = aggregated_metrics[\n",
    "                (aggregated_metrics[\"id_model\"] == eval_req.id_model)\n",
    "                & (aggregated_metrics[\"eval_handler\"] == eval_req.eval_handler)\n",
    "                & (aggregated_metrics[\"dataset_id\"] == eval_req.dataset_id)\n",
    "                & (aggregated_metrics[\"interval_start\"] == pd.to_datetime(eval_req.interval_start, unit=\"s\"))\n",
    "                & (aggregated_metrics[\"interval_end\"] == pd.to_datetime(eval_req.interval_end, unit=\"s\"))\n",
    "                & (aggregated_metrics[\"metric\"] == metric['name'])\n",
    "            ]\n",
    "            assert len(lookup) == 1, f\"Primary key not unique: {metric['name']}\"\n",
    "            metric[\"result\"] = float(lookup[\"value\"])\n",
    "        # print(log.info.eval_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_single[\n",
    "    (df_eval_single[\"id_model\"] == 2)\n",
    "    & (df_eval_single[\"eval_handler\"] == \"exactmatrix\")\n",
    "    & (df_eval_single[\"dataset_id\"] == \"cglm_hierarchical_min25-test\")\n",
    "    & (df_eval_single[\"interval_start\"] == \"2004-01-01\")\n",
    "    & (df_eval_single[\"interval_end\"] == \"2004-12-31\")\n",
    "    & (df_eval_single[\"metric\"] == \"Accuracy\")\n",
    "]\n",
    "# [\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert pd.to_datetime(\"2004-01-01\").timestamp() == 1072915200\n",
    "assert pd.to_datetime(\"2004-12-31\").timestamp() == 1104451200\n",
    "\n",
    "for log in aggregated_logs.supervisor_logs.stage_runs:\n",
    "    if log.id == PipelineStage.EVALUATE_SINGLE.name:\n",
    "        assert isinstance(log.info, SingleEvaluationInfo)\n",
    "        if not log.info.results:\n",
    "            continue\n",
    "        if (\n",
    "            (log.info.eval_request.id_model != 2)\n",
    "            or (log.info.eval_request.eval_handler != \"exactmatrix\")\n",
    "            or (log.info.eval_request.dataset_id != \"cglm_hierarchical_min25-test\")\n",
    "            or (log.info.eval_request.interval_start != 1072915200)\n",
    "            or (log.info.eval_request.interval_end != 1104451200)\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        for metric in log.info.results[\"metrics\"]:\n",
    "            if metric[\"name\"] == \"Accuracy\":\n",
    "                print(metric)\n",
    "                assert metric[\"result\"] == (0.1 - 0.05 + 0) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_logs.materialize(aggregated_log_path, mode=\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_logs.supervisor_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched pipeline\n",
    "\n",
    "Given a directory containing pipeline logs and an output path, this merges all pipelines that are identical except for the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"/Users/mboether/phd/dynamic-data/sigmod-data/cglm-landmark/data_selection_50%/logs\")\n",
    "output = Path(\"/Users/mboether/phd/dynamic-data/sigmod-data/cglm-landmark/data_selection_50%/mean_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfiles = [logfile for logfile in log_dir.glob(\"**/pipeline.log\") if (logfile.parent / \"snapshot\").exists()] # we only want pipeline.logs where we have a snapshot subdirectory, other files are probably re-executed evaluations\n",
    "\n",
    "if output.exists():\n",
    "    raise RuntimeError(f\"{output} ttalready exists, we will not override data.\")\n",
    "\n",
    "output.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "name_path_map = defaultdict(list)\n",
    "for logfile in logfiles:\n",
    "    name = (logfile.parent / \".name\").read_text()\n",
    "    log = PipelineLogs.model_validate_json(logfile.read_text())\n",
    "    name_path_map[name].append(log)\n",
    "\n",
    "for idx, (pipeline_name, logs) in enumerate(name_path_map.items()):\n",
    "    print(f\"Processing {len(logs)} runs for {pipeline_name}\")\n",
    "    pl_output = output / f\"pipeline_{idx}\"\n",
    "    pl_output.mkdir(parents=True, exist_ok=False)\n",
    "    \n",
    "    # Step 1: validate that all pipelines are the same except for the ssed\n",
    "    candidates = [\n",
    "        deepcopy(log) for log in logs\n",
    "    ]\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        candidate.config.pipeline.training.seed = candidates[0].config.pipeline.training.seed\n",
    "    \n",
    "    assert all(\n",
    "        [candidate.config == candidates[0].config for candidate in candidates]\n",
    "    ), \"Not all pipelines are the same (ignoring seed)\"\n",
    "\n",
    "    dfs_logs = [\n",
    "        logs_dataframe(log)\n",
    "        for log in logs\n",
    "    ]\n",
    "    max_sample_time = max([\n",
    "        df[\"sample_time\"].max()\n",
    "        for df in dfs_logs\n",
    "    ])\n",
    "    dfs_models_evals: list[str, tuple[pd.DataFrame, pd.DataFrame | None, pd.DataFrame | None]] = [\n",
    "        dfs_models_and_evals(log, max_sample_time)\n",
    "        for log in logs\n",
    "    ]\n",
    "    df_eval_single = pd.concat(\n",
    "        [\n",
    "            _single_eval_df\n",
    "            for _, _, _single_eval_df in dfs_models_evals\n",
    "            if _single_eval_df is not None\n",
    "        ]\n",
    "    )\n",
    "    groups = df_eval_single.groupby(\n",
    "        [\"model_idx\", \"eval_handler\", \"dataset_id\", \"interval_start\", \"interval_end\", \"metric\"]\n",
    "    )\n",
    "\n",
    "    for size in groups.size():\n",
    "        assert size == len(logs), f\"Wrong primary key: {size}, {len(logs)}\"\n",
    "    \n",
    "    aggregated_metrics = groups.agg({\n",
    "        \"value\": \"mean\"\n",
    "    }).reset_index()\n",
    "\n",
    "    _, _, first_df = dfs_models_evals[0]\n",
    "    id_model_map_df = first_df[[\"id_model\", \"model_idx\"]]\n",
    "\n",
    "    aggregated_logs = deepcopy(logs[0])\n",
    "    for log in aggregated_logs.supervisor_logs.stage_runs:\n",
    "        if log.id == PipelineStage.EVALUATE_SINGLE.name:\n",
    "            assert isinstance(log.info, SingleEvaluationInfo)\n",
    "            if not log.info.results:\n",
    "                continue\n",
    "    \n",
    "            eval_req = log.info.eval_request\n",
    "    \n",
    "            # find aggregated value\n",
    "            for metric in log.info.results[\"metrics\"]:\n",
    "                # This is broken\n",
    "                model_idx = id_model_map_df[id_model_map_df[\"model_idx\"] == eval_req.id_model].iloc[0].model_idx\n",
    "                \n",
    "                lookup = aggregated_metrics[\n",
    "                    (aggregated_metrics[\"model_idx\"] == model_idx)\n",
    "                    & (aggregated_metrics[\"eval_handler\"] == eval_req.eval_handler)\n",
    "                    & (aggregated_metrics[\"dataset_id\"] == eval_req.dataset_id)\n",
    "                    & (aggregated_metrics[\"interval_start\"] == pd.to_datetime(eval_req.interval_start, unit=\"s\"))\n",
    "                    & (aggregated_metrics[\"interval_end\"] == pd.to_datetime(eval_req.interval_end, unit=\"s\"))\n",
    "                    & (aggregated_metrics[\"metric\"] == metric['name'])\n",
    "                ]\n",
    "                assert len(lookup) == 1, f\"Primary key not unique: {metric['name']}\"\n",
    "                metric[\"result\"] = float(lookup[\"value\"])\n",
    "                \n",
    "    aggregated_logs.materialize(pl_output, mode=\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df[first_df[\"model_idx\"] == 1]\n",
    "a = first_df[[\"id_model\", \"model_idx\"]]\n",
    "a[a[\"model_idx\"] == 1].iloc[0].id_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
