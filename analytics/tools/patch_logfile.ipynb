{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "This notebooks serves a simple logfile patching purpose. As there are different ways to define the interval when a model\n",
    "is the `most recent` model for a certain interval, we allow patching the logfile to the desired definition.\n",
    "\n",
    "By default our pipeline assumes a model is most recent for the time AFTER the training interval.\n",
    "However, sometimes we want to consider the model most recent for the time DURING the training interval.\n",
    "\n",
    "This script mutates the `most_recent_model` field in the logfile to the non-default \n",
    "definition (during training interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modyn.supervisor.internal.pipeline_executor.models import PipelineLogs\n",
    "\n",
    "\n",
    "from modyn.supervisor.internal.grpc.enums import PipelineStage\n",
    "# fill missing times in cumulative plot\n",
    "\n",
    "\n",
    "from analytics.app.data.transform import logs_dataframe\n",
    "from pathlib import Path\n",
    "from analytics.app.data.transform import dfs_models_and_evals\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "\n",
    "pipeline_logfile = Path(\"/Users/robinholzinger/robin/dev/eth/modyn-2/.analytics.log/.data/pipeline_6o/pipeline.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = PipelineLogs.model_validate_json(pipeline_logfile.read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = [(l_ for l_ in logs.supervisor_logs.stage_runs if l_.id == PipelineStage.HANDLE_SINGLE_TRIGGER.name)]\n",
    "evals = [(l_ for l_ in logs.supervisor_logs.stage_runs if l_.id == PipelineStage.EVALUATE_SINGLE.name and l_.info.eval_request.dataset_id == \"cglm_landmark_min25-test\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytics.app.data.transform import dfs_models_and_evals\n",
    "\n",
    "df_logs = logs_dataframe(logs)\n",
    "# max_timestamp = df_logs[\"sample_time\"].max()\n",
    "max_timestamp = df_logs[\"sample_time\"].max()\n",
    "df_models, eval_requests, evals_metrics = dfs_models_and_evals(logs, max_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_requests[eval_requests[\"currently_active_model\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch logfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_log in logs.supervisor_logs.stage_runs:\n",
    "    if eval_log.id == PipelineStage.EVALUATE_SINGLE.name:\n",
    "        # Let's throw away all information about the most recent model, let's rebuild it\n",
    "        eval_log.info.eval_request.currently_active_model = False\n",
    "\n",
    "        # For a fixed interval the evaluation request of a certain model is the most recent, if the model training\n",
    "        # interval center lies within the evaluation interval.\n",
    "        # Note: this is not a generic solution, but works for the slicing case with fixed evaluation and trigger\n",
    "        # intervals in the same order of magnitude.\n",
    "        model_row = df_models[df_models[\"id_model\"] == eval_log.info.eval_request.id_model]\n",
    "        assert len(model_row) == 1\n",
    "\n",
    "        training_center = (model_row.iloc[0][\"train_start\"].to_pydatetime().timestamp() + model_row.iloc[0][\"train_end\"].to_pydatetime().timestamp()) / 2\n",
    "        eval_log.info.eval_request.currently_active_model = eval_log.info.eval_request.interval_start <= training_center <= eval_log.info.eval_request.interval_end\n",
    "        eval_log.info.eval_request.currently_trained_model = eval_log.info.eval_request.interval_start <= training_center <= eval_log.info.eval_request.interval_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results back\n",
    "pipeline_logfile.write_text(logs.model_dump_json(by_alias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patch_logfile(path):\n",
    "    logs = PipelineLogs.model_validate_json(path.read_text())\n",
    "    trains = [(l_ for l_ in logs.supervisor_logs.stage_runs if l_.id == PipelineStage.HANDLE_SINGLE_TRIGGER.name)]\n",
    "    evals = [(l_ for l_ in logs.supervisor_logs.stage_runs if l_.id == PipelineStage.EVALUATE_SINGLE.name and l_.info.eval_request.dataset_id == \"cglm_landmark_min25-test\")]\n",
    "    df_logs = logs_dataframe(logs)\n",
    "    max_timestamp = df_logs[\"sample_time\"].max()\n",
    "    df_models, eval_requests, evals_metrics = dfs_models_and_evals(logs, max_timestamp)\n",
    "\n",
    "    for eval_log in logs.supervisor_logs.stage_runs:\n",
    "        if eval_log.id == PipelineStage.EVALUATE_SINGLE.name:\n",
    "            # Let's throw away all information about the most recent model, let's rebuild it\n",
    "            eval_log.info.eval_request.currently_active_model = False\n",
    "    \n",
    "            # For a fixed interval the evaluation request of a certain model is the most recent, if the model training\n",
    "            # interval center lies within the evaluation interval.\n",
    "            # Note: this is not a generic solution, but works for the slicing case with fixed evaluation and trigger\n",
    "            # intervals in the same order of magnitude.\n",
    "            model_row = df_models[df_models[\"id_model\"] == eval_log.info.eval_request.id_model]\n",
    "            assert len(model_row) == 1\n",
    "    \n",
    "            training_center = (model_row.iloc[0][\"train_start\"].to_pydatetime().timestamp() + model_row.iloc[0][\"train_end\"].to_pydatetime().timestamp()) / 2\n",
    "            eval_log.info.eval_request.currently_active_model = eval_log.info.eval_request.interval_start <= training_center <= eval_log.info.eval_request.interval_end\n",
    "            eval_log.info.eval_request.currently_trained_model = eval_log.info.eval_request.interval_start <= training_center <= eval_log.info.eval_request.interval_end\n",
    "\n",
    "    patched_path = path.parent / \"pipeline.patched\"\n",
    "    patched_path.write_text(logs.model_dump_json(by_alias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"/Users/mboether/phd/dynamic-data/sigmod-data/cglm-landmark/data_selection_50%/logs\")\n",
    "logfiles = [logfile for logfile in log_dir.glob(\"**/pipeline.log\") if (logfile.parent / \"snapshot\").exists()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for logfile in tqdm(logfiles):\n",
    "    patch_logfile(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
