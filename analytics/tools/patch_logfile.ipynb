{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "This notebooks serves a simple logfile patching purpose. As there are different ways to define the interval when a model\n",
    "is the `most recent` model for a certain interval, we allow patching the logfile to the desired definition.\n",
    "\n",
    "By default our pipeline assumes a model is most recent for the time AFTER the training interval.\n",
    "However, sometimes we want to consider the model most recent for the time DURING the training interval.\n",
    "\n",
    "This script mutates the `most_recent_model` field in the logfile to the non-default \n",
    "definition (during training interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from modyn.supervisor.internal.pipeline_executor.models import PipelineLogs\n",
    "\n",
    "\n",
    "from modyn.supervisor.internal.grpc.enums import PipelineStage\n",
    "# fill missing times in cumulative plot\n",
    "\n",
    "\n",
    "from analytics.app.data.transform import logs_dataframe\n",
    "from pathlib import Path\n",
    "from analytics.app.data.transform import dfs_models_and_evals\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "\n",
    "pipeline_logfile = Path(\"/Users/mboether/phd/dynamic-data/sigmod-data/criteo/throughput/logs_tput_run1/pipeline_8/pipeline.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = PipelineLogs.model_validate_json(pipeline_logfile.read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# max_timestamp = df_logs[\"sample_time\"].max()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m max_timestamp \u001b[38;5;241m=\u001b[39m df_logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m----> 4\u001b[0m df_models, eval_requests, evals_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mdfs_models_and_evals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_timestamp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/phd/dynamic-data/dynamic_datasets_dsl/analytics/app/data/transform.py:132\u001b[0m, in \u001b[0;36mdfs_models_and_evals\u001b[0;34m(logs, max_sample_time, pipeline_ref)\u001b[0m\n\u001b[1;32m    128\u001b[0m df_models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline_ref\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pipeline_ref\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------- EVALUATIONS ------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m dfs_requests \u001b[38;5;241m=\u001b[39m \u001b[43mStageLog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupervisor_logs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage_runs\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPipelineStage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEVALUATE_SINGLE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfailure_reason\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_request\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m dfs_metrics \u001b[38;5;241m=\u001b[39m SingleEvaluationInfo\u001b[38;5;241m.\u001b[39mresults_df(\n\u001b[1;32m    146\u001b[0m     (\n\u001b[1;32m    147\u001b[0m         cast(SingleEvaluationInfo, run\u001b[38;5;241m.\u001b[39minfo)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dfs_requests\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dfs_metrics\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/phd/dynamic-data/dynamic_datasets_dsl/modyn/supervisor/internal/pipeline_executor/models.py:477\u001b[0m, in \u001b[0;36mStageLog.df\u001b[0;34m(cls, stage_logs, extended)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m    474\u001b[0m stage_logs, iter_copy \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mtee(stage_logs)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    476\u001b[0m     [stage\u001b[38;5;241m.\u001b[39mdf_row(extended\u001b[38;5;241m=\u001b[39mextended) \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m stage_logs],\n\u001b[0;32m--> 477\u001b[0m     columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_copy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdf_columns(extended\u001b[38;5;241m=\u001b[39mextended),\n\u001b[1;32m    478\u001b[0m )\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "df_logs = logs_dataframe(logs)\n",
    "# max_timestamp = df_logs[\"sample_time\"].max()\n",
    "max_timestamp = df_logs[\"sample_time\"].max()\n",
    "df_models, eval_requests, evals_metrics = dfs_models_and_evals(logs, max_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_requests[eval_requests[\"currently_active_model\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch logfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_log in logs.supervisor_logs.stage_runs:\n",
    "    if eval_log.id == PipelineStage.EVALUATE_SINGLE.name:\n",
    "        # For a fixed interval the evaluation request of a certain model is the most recent, if the model training\n",
    "        # interval center lies within the evaluation interval.\n",
    "        # Note: this is not a generic solution, but works for the slicing case with fixed evaluation and trigger\n",
    "        # intervals in the same order of magnitude.\n",
    "        model_row = df_models[df_models[\"id_model\"] == eval_log.info.eval_request.id_model]\n",
    "        assert len(model_row) == 1\n",
    "\n",
    "        training_center = (model_row.iloc[0][\"train_start\"].to_pydatetime().timestamp() + model_row.iloc[0][\"train_end\"].to_pydatetime().timestamp()) / 2\n",
    "        eval_log.info.eval_request.currently_trained_model = eval_log.info.eval_request.interval_start <= training_center <= eval_log.info.eval_request.interval_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results back\n",
    "pipeline_logfile.write_text(logs.model_dump_json(by_alias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_logfile(path, triggers_to_delete=[]):\n",
    "    logs = PipelineLogs.model_validate_json(path.read_text())\n",
    "    df_logs = logs_dataframe(logs)\n",
    "    max_timestamp = df_logs[\"sample_time\"].max()\n",
    "    df_models, eval_requests, evals_metrics = dfs_models_and_evals(logs, max_timestamp)\n",
    "\n",
    "    idx_to_delete = []\n",
    "    for stage_run_idx, eval_log in enumerate(logs.supervisor_logs.stage_runs):\n",
    "        if eval_log.id == PipelineStage.EVALUATE_SINGLE.name:\n",
    "            # For a fixed interval the evaluation request of a certain model is the most recent, if the model training\n",
    "            # interval center lies within the evaluation interval.\n",
    "            # Note: this is not a generic solution, but works for the slicing case with fixed evaluation and trigger\n",
    "            # intervals in the same order of magnitude.\n",
    "            model_row = df_models[df_models[\"id_model\"] == eval_log.info.eval_request.id_model]\n",
    "            assert len(model_row) == 1\n",
    "    \n",
    "            training_center = (model_row.iloc[0][\"train_start\"].to_pydatetime().timestamp() + model_row.iloc[0][\"train_end\"].to_pydatetime().timestamp()) / 2\n",
    "            eval_log.info.eval_request.currently_trained_model = eval_log.info.eval_request.interval_start <= training_center <= eval_log.info.eval_request.interval_end\n",
    "\n",
    "            if eval_log.info.eval_request.trigger_id in triggers_to_delete:\n",
    "                idx_to_delete.append(stage_run_idx)\n",
    "\n",
    "    logs.supervisor_logs.stage_runs = [log for idx,log in enumerate(logs.supervisor_logs.stage_runs) if idx not in idx_to_delete]\n",
    "    \n",
    "    patched_path = path.parent / \"pipeline.patched\"\n",
    "    patched_path.write_text(logs.model_dump_json(by_alias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"/Users/mboether/phd/dynamic-data/sigmod-data/cglm-hierarchical/data_selection/logs_agg\")\n",
    "triggers_to_delete = [1,16]\n",
    "logfiles = [logfile for logfile in log_dir.glob(\"**/pipeline.log\")] # if (logfile.parent / \"snapshot\").exists()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:06<00:00,  4.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for logfile in tqdm(logfiles):\n",
    "    patch_logfile(logfile, triggers_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_red = df_models[[\"trigger_id\", \"id_model\", \"train_start\", \"train_end\"]]\n",
    "models_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_red = eval_requests[[\"trigger_id\", \"training_idx\", \"model_idx\", \"interval_start\", \"interval_end\", \"eval_handler\", \"dataset_id\"]]\n",
    "eval_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross = models_red.merge(eval_red, on=\"trigger_id\").rename(columns={\"train_start\": \"first_timestamp\", \"train_end\": \"last_timestamp\"})\n",
    "assert df_cross.shape[0] == eval_red.shape[0]\n",
    "df_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted logic from handler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cross[\"active_candidate\"] = df_cross[\"last_timestamp\"] < df_cross[\"active_model_trained_before\"]\n",
    "\n",
    "# # find the maximum model for every EvalCandidate that doesn't violate that constraint\n",
    "# max_model_id = (\n",
    "#     df_cross[df_cross[\"active_candidate\"]]\n",
    "#     .groupby(\"active_model_trained_before\")[\"id_model\"]\n",
    "#     .aggregate(max_model_id=\"max\")\n",
    "# )\n",
    "\n",
    "# # combine: a model in the cross product is most recent for a certain interval iff\n",
    "# #  it has maximum model id for its active_model_trained_before\n",
    "# df_active_models = df_cross.merge(max_model_id, on=\"active_model_trained_before\", how=\"left\")\n",
    "# df_active_models[\"active_model\"] = df_active_models[\"id_model\"] == df_active_models[\"max_model_id\"]\n",
    "\n",
    "# # for a given interval, the currently trained model is the model with the smallest id\n",
    "# # from all models that have a strictly bigger id than the most recent model. Hence it is the model after the\n",
    "# # most recent model.\n",
    "# # For that we first build a model -> successor model mapping:\n",
    "# model_successor_relation = df_active_models[[\"id_model\"]].drop_duplicates().sort_values(by=\"id_model\")\n",
    "# model_successor_relation[\"next_id_model\"] = model_successor_relation[\"id_model\"].shift(-1, fill_value=-1)\n",
    "\n",
    "# # if there's no active model for the first interval(s), we still need to define the next model as the\n",
    "# # trained model\n",
    "# model_successor_relation = pd.concat(\n",
    "#     [\n",
    "#         model_successor_relation,\n",
    "#         pd.DataFrame([{\"id_model\": None, \"next_id_model\": df_active_models[\"id_model\"].min()}]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# df_trained_models = df_active_models.merge(\n",
    "#     model_successor_relation, how=\"left\", left_on=\"max_model_id\", right_on=\"id_model\", suffixes=(\"\", \"__\")\n",
    "# )\n",
    "# df_trained_models[\"trained_model\"] = df_trained_models[\"id_model\"] == df_trained_models[\"next_id_model\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
