{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "This notebooks serves a simple logfile patching purpose. As there are different ways to define the interval when a model\n",
    "is the `most recent` model for a certain interval, we allow patching the logfile to the desired definition.\n",
    "\n",
    "By default our pipeline assumes a model is most recent for the time AFTER the training interval.\n",
    "However, sometimes we want to consider the model most recent for the time DURING the training interval.\n",
    "\n",
    "This script mutates the `most_recent_model` field in the logfile to the non-default \n",
    "definition (during training interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from analytics.app.data.transform import dfs_models_and_evals, logs_dataframe\n",
    "from modyn.supervisor.internal.grpc.enums import PipelineStage\n",
    "\n",
    "# fill missing times in cumulative plot\n",
    "from modyn.supervisor.internal.pipeline_executor.models import MultiEvaluationInfo, PipelineLogs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "\n",
    "pipeline_logfile = Path(\"/Users/robinholzinger/robin/dev/eth/modyn-2/.data/test/pipeline_8/pipeline.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = PipelineLogs.model_validate_json(pipeline_logfile.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs = logs_dataframe(logs)\n",
    "# max_timestamp = df_logs[\"sample_time\"].max()\n",
    "max_timestamp = df_logs[\"sample_time\"].max()\n",
    "df_models, eval_requests, evals_metrics = dfs_models_and_evals(logs, max_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_requests[eval_requests[\"currently_active_model\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch logfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modyn.utils.utils import SECONDS_PER_UNIT\n",
    "\n",
    "offset = SECONDS_PER_UNIT[\"w\"] * 25 * 2\n",
    "for eval_log in logs.supervisor_logs.stage_runs:\n",
    "    if eval_log.id == PipelineStage.EVALUATE_MULTI.name:\n",
    "        # For a fixed interval the evaluation request of a certain model is the most recent, if the model training\n",
    "        # interval center lies within the evaluation interval.\n",
    "        # Note: this is not a generic solution, but works for the slicing case with fixed evaluation and trigger\n",
    "        # intervals in the same order of magnitude.\n",
    "        assert isinstance(eval_log.info, MultiEvaluationInfo)\n",
    "\n",
    "        for single_info in eval_log.info.interval_results:\n",
    "            eval_request = single_info.eval_request\n",
    "\n",
    "            model_row = df_models[df_models[\"id_model\"] == eval_request.id_model]\n",
    "            assert len(model_row) == 1\n",
    "\n",
    "            evaluation_center = (eval_request.interval_start + eval_request.interval_end) / 2\n",
    "\n",
    "            eval_request.currently_active_model = (\n",
    "                evaluation_center - offset\n",
    "                <= model_row.iloc[0][\"train_end\"].to_pydatetime().timestamp()\n",
    "                <= evaluation_center\n",
    "            )\n",
    "\n",
    "            # training_center = (\n",
    "            #     model_row.iloc[0][\"train_start\"].to_pydatetime().timestamp()\n",
    "            #     + model_row.iloc[0][\"train_end\"].to_pydatetime().timestamp()\n",
    "            # ) / 2\n",
    "            # eval_request.currently_active_model = eval_request.currently_trained_model\n",
    "            # eval_request.currently_trained_model = (\n",
    "            #     eval_request.interval_start <= training_center <= eval_request.interval_end\n",
    "            # )\n",
    "\n",
    "new_df_models, new_eval_requests, new_evals_metrics = dfs_models_and_evals(logs, max_timestamp)\n",
    "\n",
    "\n",
    "# # # Another pass for the currently trained model\n",
    "# for eval_log in logs.supervisor_logs.stage_runs:\n",
    "#     if eval_log.id == PipelineStage.EVALUATE_MULTI.name:\n",
    "#         assert len(model_row) == 1\n",
    "\n",
    "#         found = new_eval_requests[\n",
    "#             (\n",
    "#                 new_eval_requests[\"id_model\"] == eval_request.id_model + 1\n",
    "#             ) & (\n",
    "#                 new_eval_requests[\"interval_start\"] == pd.to_datetime(eval_request.interval_start, unit=\"s\")\n",
    "#             ) &\n",
    "#             (\n",
    "#                 new_eval_requests[\"interval_end\"] == pd.to_datetime(eval_request.interval_end, unit=\"s\")\n",
    "#             ) & (\n",
    "#                 new_eval_requests[\"currently_active_model\"] == True\n",
    "#             )\n",
    "#         ]\n",
    "#         eval_request.currently_trained_model = len(found) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results back\n",
    "Path(\"/Users/robinholzinger/robin/dev/eth/modyn-2/.data/test/pipeline_9/pipeline.log\").write_text(\n",
    "    logs.model_dump_json(by_alias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_logfile(path: Path):\n",
    "    logs = PipelineLogs.model_validate_json(path.read_text())\n",
    "    df_logs = logs_dataframe(logs)\n",
    "    max_timestamp = df_logs[\"sample_time\"].max()\n",
    "    df_models, eval_requests, evals_metrics = dfs_models_and_evals(logs, max_timestamp)\n",
    "\n",
    "    for eval_log in logs.supervisor_logs.stage_runs:\n",
    "        if eval_log.id == PipelineStage.EVALUATE_MULTI.name:\n",
    "            # For a fixed interval the evaluation request of a certain model is the most recent, if the model training\n",
    "            # interval center lies within the evaluation interval.\n",
    "            # Note: this is not a generic solution, but works for the slicing case with fixed evaluation and trigger\n",
    "            # intervals in the same order of magnitude.\n",
    "            assert isinstance(eval_log.info, MultiEvaluationInfo)\n",
    "\n",
    "            for single_info in eval_log.info.interval_results:\n",
    "                eval_request = single_info.eval_request\n",
    "\n",
    "                model_row = df_models[df_models[\"id_model\"] == eval_request.id_model]\n",
    "                assert len(model_row) == 1\n",
    "\n",
    "                training_center = (\n",
    "                    model_row.iloc[0][\"train_start\"].to_pydatetime().timestamp()\n",
    "                    + model_row.iloc[0][\"train_end\"].to_pydatetime().timestamp()\n",
    "                ) / 2\n",
    "                eval_request.currently_active_model = eval_request.currently_trained_model\n",
    "                eval_request.currently_trained_model = (\n",
    "                    eval_request.interval_start <= training_center <= eval_request.interval_end\n",
    "                )\n",
    "\n",
    "    path.write_text(logs.model_dump_json(by_alias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\n",
    "    \"/Users/robinholzinger/robin/dev/eth/modyn-sigmod-data/cglm-landmark/data_selection/logs_agg_patch_currently_trained\"\n",
    ")\n",
    "logfiles = [logfile for logfile in log_dir.glob(\"**/pipeline.log\")]\n",
    "logfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for logfile in tqdm(logfiles):\n",
    "    patch_logfile(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_red = df_models[[\"trigger_id\", \"id_model\", \"train_start\", \"train_end\"]]\n",
    "models_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_red = eval_requests[\n",
    "    [\"trigger_id\", \"training_idx\", \"model_idx\", \"interval_start\", \"interval_end\", \"eval_handler\", \"dataset_id\"]\n",
    "]\n",
    "eval_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross = models_red.merge(eval_red, on=\"trigger_id\").rename(\n",
    "    columns={\"train_start\": \"first_timestamp\", \"train_end\": \"last_timestamp\"}\n",
    ")\n",
    "assert df_cross.shape[0] == eval_red.shape[0]\n",
    "df_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted logic from handler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cross[\"active_candidate\"] = df_cross[\"last_timestamp\"] < df_cross[\"active_model_trained_before\"]\n",
    "\n",
    "# # find the maximum model for every EvalCandidate that doesn't violate that constraint\n",
    "# max_model_id = (\n",
    "#     df_cross[df_cross[\"active_candidate\"]]\n",
    "#     .groupby(\"active_model_trained_before\")[\"id_model\"]\n",
    "#     .aggregate(max_model_id=\"max\")\n",
    "# )\n",
    "\n",
    "# # combine: a model in the cross product is most recent for a certain interval iff\n",
    "# #  it has maximum model id for its active_model_trained_before\n",
    "# df_active_models = df_cross.merge(max_model_id, on=\"active_model_trained_before\", how=\"left\")\n",
    "# df_active_models[\"active_model\"] = df_active_models[\"id_model\"] == df_active_models[\"max_model_id\"]\n",
    "\n",
    "# # for a given interval, the currently trained model is the model with the smallest id\n",
    "# # from all models that have a strictly bigger id than the most recent model. Hence it is the model after the\n",
    "# # most recent model.\n",
    "# # For that we first build a model -> successor model mapping:\n",
    "# model_successor_relation = df_active_models[[\"id_model\"]].drop_duplicates().sort_values(by=\"id_model\")\n",
    "# model_successor_relation[\"next_id_model\"] = model_successor_relation[\"id_model\"].shift(-1, fill_value=-1)\n",
    "\n",
    "# # if there's no active model for the first interval(s), we still need to define the next model as the\n",
    "# # trained model\n",
    "# model_successor_relation = pd.concat(\n",
    "#     [\n",
    "#         model_successor_relation,\n",
    "#         pd.DataFrame([{\"id_model\": None, \"next_id_model\": df_active_models[\"id_model\"].min()}]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# df_trained_models = df_active_models.merge(\n",
    "#     model_successor_relation, how=\"left\", left_on=\"max_model_id\", right_on=\"id_model\", suffixes=(\"\", \"__\")\n",
    "# )\n",
    "# df_trained_models[\"trained_model\"] = df_trained_models[\"id_model\"] == df_trained_models[\"next_id_model\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
