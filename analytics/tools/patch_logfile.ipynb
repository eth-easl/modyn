{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "This notebooks serves a simple logfile patching purpose. As there are different ways to define the interval when a model\n",
    "is the `most recent` model for a certain interval, we allow patching the logfile to the desired definition.\n",
    "\n",
    "By default our pipeline assumes a model is most recent for the time AFTER the training interval.\n",
    "However, sometimes we want to consider the model most recent for the time DURING the training interval.\n",
    "\n",
    "This script mutates the `most_recent_model` field in the logfile to the non-default \n",
    "definition (during training interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modyn.supervisor.internal.pipeline_executor.models import PipelineLogs\n",
    "\n",
    "\n",
    "from modyn.supervisor.internal.grpc.enums import PipelineStage\n",
    "# fill missing times in cumulative plot\n",
    "\n",
    "\n",
    "from analytics.app.data.transform import logs_dataframe\n",
    "from pathlib import Path\n",
    "from analytics.app.data.transform import dfs_models_and_evals\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "\n",
    "pipeline_logfile = Path(\"/Users/robinholzinger/robin/dev/eth/modyn-2/.analytics.log/.data/pipeline_11/pipeline.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = PipelineLogs.model_validate_json(pipeline_logfile.read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_logs = logs_dataframe(logs)\n",
    "# max_timestamp = df_logs[\"sample_time\"].max()\n",
    "max_timestamp = df_logs[\"sample_time\"].max()\n",
    "df_models, eval_requests, evals_metrics = dfs_models_and_evals(logs, max_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_requests[eval_requests[\"currently_active_model\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch logfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_log in logs.supervisor_logs.stage_runs:\n",
    "    if eval_log.id == PipelineStage.EVALUATE_MULTI.name:\n",
    "        # For a fixed interval the evaluation request of a certain model is the most recent, if the model training\n",
    "        # interval center lies within the evaluation interval.\n",
    "        # Note: this is not a generic solution, but works for the slicing case with fixed evaluation and trigger\n",
    "        # intervals in the same order of magnitude.\n",
    "        model_row = df_models[df_models[\"id_model\"] == eval_log.info.eval_request.id_model]\n",
    "        assert len(model_row) == 1\n",
    "\n",
    "        training_center = (model_row.iloc[0][\"train_start\"].to_pydatetime().timestamp() + model_row.iloc[0][\"train_end\"].to_pydatetime().timestamp()) / 2\n",
    "        eval_log.info.eval_request.currently_trained_model = eval_log.info.eval_request.interval_start <= training_center <= eval_log.info.eval_request.interval_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results back\n",
    "pipeline_logfile.write_text(logs.model_dump_json(by_alias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patch_logfile(path):\n",
    "    logs = PipelineLogs.model_validate_json(path.read_text())\n",
    "    df_logs = logs_dataframe(logs)\n",
    "    max_timestamp = df_logs[\"sample_time\"].max()\n",
    "    df_models, eval_requests, evals_metrics = dfs_models_and_evals(logs, max_timestamp)\n",
    "\n",
    "    for eval_log in logs.supervisor_logs.stage_runs:\n",
    "        if eval_log.id == PipelineStage.EVALUATE_MULTI.name:\n",
    "            # Let's throw away all information about the most recent model, let's rebuild it\n",
    "            eval_log.info.eval_request.currently_active_model = False\n",
    "\n",
    "            # For a fixed interval the evaluation request of a certain model is the most recent, if the model training\n",
    "            # interval center lies within the evaluation interval.\n",
    "            # Note: this is not a generic solution, but works for the slicing case with fixed evaluation and trigger\n",
    "            # intervals in the same order of magnitude.\n",
    "            model_row = df_models[df_models[\"id_model\"] == eval_log.info.eval_request.id_model]\n",
    "            assert len(model_row) == 1\n",
    "\n",
    "            training_center = (model_row.iloc[0][\"train_start\"].to_pydatetime().timestamp() + model_row.iloc[0][\"train_end\"].to_pydatetime().timestamp()) / 2\n",
    "            eval_log.info.eval_request.currently_active_model = eval_log.info.eval_request.interval_start <= training_center <= eval_log.info.eval_request.interval_end\n",
    "            eval_log.info.eval_request.currently_trained_model = eval_log.info.eval_request.interval_start <= training_center <= eval_log.info.eval_request.interval_end\n",
    "\n",
    "    patched_path = path.parent / \"pipeline.patched\"\n",
    "    patched_path.write_text(logs.model_dump_json(by_alias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"/Users/mboether/phd/dynamic-data/sigmod-data/cglm-landmark/data_selection_50%/logs\")\n",
    "logfiles = [logfile for logfile in log_dir.glob(\"**/pipeline.log\") if (logfile.parent / \"snapshot\").exists()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for logfile in tqdm(logfiles):\n",
    "    patch_logfile(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_red = df_models[[\"trigger_id\", \"id_model\", \"train_start\", \"train_end\"]]\n",
    "models_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_red = eval_requests[[\"trigger_id\", \"training_idx\", \"model_idx\", \"interval_start\", \"interval_end\", \"eval_handler\", \"dataset_id\"]]\n",
    "eval_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross = models_red.merge(eval_red, on=\"trigger_id\").rename(columns={\"train_start\": \"first_timestamp\", \"train_end\": \"last_timestamp\"})\n",
    "assert df_cross.shape[0] == eval_red.shape[0]\n",
    "df_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted logic from handler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cross[\"active_candidate\"] = df_cross[\"last_timestamp\"] < df_cross[\"active_model_trained_before\"]\n",
    "\n",
    "# # find the maximum model for every EvalCandidate that doesn't violate that constraint\n",
    "# max_model_id = (\n",
    "#     df_cross[df_cross[\"active_candidate\"]]\n",
    "#     .groupby(\"active_model_trained_before\")[\"id_model\"]\n",
    "#     .aggregate(max_model_id=\"max\")\n",
    "# )\n",
    "\n",
    "# # combine: a model in the cross product is most recent for a certain interval iff\n",
    "# #  it has maximum model id for its active_model_trained_before\n",
    "# df_active_models = df_cross.merge(max_model_id, on=\"active_model_trained_before\", how=\"left\")\n",
    "# df_active_models[\"active_model\"] = df_active_models[\"id_model\"] == df_active_models[\"max_model_id\"]\n",
    "\n",
    "# # for a given interval, the currently trained model is the model with the smallest id\n",
    "# # from all models that have a strictly bigger id than the most recent model. Hence it is the model after the\n",
    "# # most recent model.\n",
    "# # For that we first build a model -> successor model mapping:\n",
    "# model_successor_relation = df_active_models[[\"id_model\"]].drop_duplicates().sort_values(by=\"id_model\")\n",
    "# model_successor_relation[\"next_id_model\"] = model_successor_relation[\"id_model\"].shift(-1, fill_value=-1)\n",
    "\n",
    "# # if there's no active model for the first interval(s), we still need to define the next model as the\n",
    "# # trained model\n",
    "# model_successor_relation = pd.concat(\n",
    "#     [\n",
    "#         model_successor_relation,\n",
    "#         pd.DataFrame([{\"id_model\": None, \"next_id_model\": df_active_models[\"id_model\"].min()}]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# df_trained_models = df_active_models.merge(\n",
    "#     model_successor_relation, how=\"left\", left_on=\"max_model_id\", right_on=\"id_model\", suffixes=(\"\", \"__\")\n",
    "# )\n",
    "# df_trained_models[\"trained_model\"] = df_trained_models[\"id_model\"] == df_trained_models[\"next_id_model\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
