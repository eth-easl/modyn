%YAML 1.1
---
$schema: "http://json-schema.org/draft-04/schema"
id: "http://stsci.edu/schemas/yaml-schema/draft-01"
title:
  Modyn Pipeline Configuration
description: |
  This is a configuration file for a pipeline to be run by Modyn.
properties:
  pipeline:
    type: object
    properties:
      name:
        type: string
        description: |
          The name of the pipeline.
      description:
        type: string
        description: |
          The description of the pipeline.
      version:
        type: string
        description: |
          The version of the pipeline.
    required:
      - name
  model:
    type: object
    properties:
      id:
        type: string
        description: |
          The ID of the model that should be trained.
      config:
        type: object
        description: |
          Configuration dictionary that will be passed to the model on initialization.
    required:
      - id
  training:
    type: object
    properties:
      gpus:
        type: number
        description: |
          The number of GPUs that should be used for training.
      epochs_per_trigger:
        type: number
        description: |
          The number of epochs per trigger. Defaults to 1, if not given.
      device:
        type: string
        description: |
          The device the model should be put on. In the future (#131), we might want this to be either "cpu" or "gpu" and let the trainer server figure out the exact device, but for now, this really is the identifier of the device.
      amp:
        type: boolean
        description: |
          If True, automatic mixed precision will be used.
      dataloader_workers:
        type: number
        description: |
          The number of data loader workers on the trainer node that fetch data from storage.
      batch_size:
        type: number
        description: |
          The batch size to be used during training.
      use_previous_model:
        type: boolean
        description: |
          If True, on trigger, we continue training on the model outputted by the previous trigger. If False, we start with random weights.
          If `initial_model` is "pretrained", cannot be False.
      selection_strategy:
        type: object
        description: |
          Configuration for the Selector
        properties:
          name:
            type: string
            description: |
              The Selector strategy that the pipeline should use. Currently supported: NewDataStrategy, FreshnessSamplingStrategy, LossDownsamplingStrategy, GradNormDownsamplingStrategy
          maximum_keys_in_memory:
            type: number
            description: |
              Limits how many keys should be materialized at a time in the strategy.
          config:
            type: object
            description: |
              Configuration dictionary that will be passed to the strategy
            properties:
              limit:
                type: number
                description: |
                  This limits how many data points we train on at maximum on a trigger. Set to -1 to disable limit.
              reset_after_trigger:
                type: boolean
                description: |
                  If set to true, the selection strategy resets its internal state after a trigger.
              tail_triggers:
                type: number
                description: |
                  For the training iteration, just use data from this trigger and the previous tail_triggers. 
                  reset_after_trigger is equivalent to tail_triggers = 0. Omit this parameter if you want to use every previous datapoint.
              unused_data_ratio:
                type: number
                description: |
                 [FreshnessSamplingStrategy] Ratio that defines how much data in the training set per trigger should be from previously unused data (in all previous triggers).
              limit_reset:
                type: string
                description: |
                 [NewDataStrategy] Strategy to follow for respecting the limit in case of reset. Only used when reset_after_trigger == true. Either "lastX" or "sampleUAR" are supported. See class description for further info.
              presampling_ratio:
                type: number
                description: |
                  [AbstractDownsampleStrategy] percentage of points on which the metric (loss, gradient norm,..) is computed. Must be between 0 and 100
              sample_then_batch:
                type: boolean
                description: |
                  [AbstractDownsampleStrategy] If True, the samples are first sampled and then batched and supplied to the training loop. If False, the datapoints are first divided into batches and then sampled.
              downsampling_period:
                type: number
                description: |
                  [AbstractDownsampleStrategy] In multi-epoch training and sample_then_batch, how frequently the data is selected. The default value is 1 (select every epoch). To select once per trigger, set this parameter to 0.
              downsampled_batch_ratio:
                type: number
                description: |
                  [AbstractDownsampleStrategy] Ratio post_sampling_size/pre_sampling_size. For example, with 160 datapoints and a ratio of 50 we keep 80. Requires sample_then_batch set to True and must be in (0,100)
          processor_type:
            type: string
            description: |
              The name of the Metadata Processor strategy that should be used.
        required:
          - name
          - maximum_keys_in_memory
      initial_model:
        type: string
        description: |
          What type of initial model should be used (random or pretrained).
      initial_model_path:
        type: string
        description: |
          In case of pretrained initial model, provide path (local on supervisor) to initial model
      initial_pass:
        type: object
        description: |
          Setting of initial pass through data
        properties:
          activated:
            type: boolean
            description: |
              Whether we do an initial pass through the data
          reference:
            type: string
            description: |
              If we do an initial pass, we define whether we reference a certain percentage of all data (= amount) or all data in an interval (= start_timestamp, end_timestamp)
        required:
          - activated
      checkpointing:
        type: object
        description: |
          Configuration of checkpointing during training
        properties:
          activated:
            type: boolean
            description: |
              Whether we checkpoint or not
          interval:
            type: number
            description: |
              In what interval we checkpoint
          path:
            type: string
            description: |
              The path on the training node where the checkpoints are stored
        required:
          - activated
      optimizers:
        type: array
        description: |
          An array of the optimizers for the training
        minItems: 1
        items:
          type: object
          description: |
            Configuration for the optimizer (e.g., Adam)
          properties:
            name:
              type: string
              description: |
                The name of the optimizer (like an ID)
            algorithm:
              type: string
              description: |
                The type of the optimizer (e.g. SGD)
            source:
              type: string
              description: The framework/package the optimizer comes from. Currently PyTorch and APEX are supported
            param_groups:
              type: array
              description: |
                An array of the parameter groups (parameters and optional configs) that this optimizer is responsible for
              minItems: 1
              items:
                type: object
                description: |
                  Configuration for a parameter group
                properties:
                  module:
                    type: string
                    description: |
                      A set of parameters
                  config:
                    type: object
                    description: |
                      Optional configuration for the parameter group (e.g. learning rate)
                required:
                  - module
          required:
            - name
            - algorithm
            - source
            - param_groups
      optimization_criterion:
        type: object
        description: |
          Configuration for the optimization criterion that we optimize
        properties:
          name:
            type: string
            description: |
              The name of the criterion that the pipeline uses (e.g., CrossEntropyLoss)
          config:
            type: object
            description: |
              Optional configuration of the criterion. Passed to the optimizer class as a dict.
        required:
          - name
      lr_scheduler:
        type: object
        description: |
          Configuration for the Torch-based Learning Rate (LR) scheduler used for training.
        properties:
          name:
            type: string
            description: The name of the LR scheduler.
          source:
            type: string
            description: Source of the LR scheduler (for now, only PyTorch and custom are supported).
          optimizers:
            type: array
            minItems: 1
            description: |
              List of optimizers that this scheduler is responsible for. In case a PyTorch LR scheduler is used, this list should have only one item.
            items:
              type: string
          config:
            type: object
            description: |
              Optional configuration of the lr scheduler. Passed to the lr scheduler as a dict.
        required:
          - name
          - source
          - optimizers
      grad_scaler_config:
        type: object
        description: |
          Configuration for the torch.cuda.amp.GradScaler. Effective only when amp is enabled.
    required:
      - gpus
      - device
      - dataloader_workers
      - batch_size
      - selection_strategy
      - initial_model
      - initial_pass
      - use_previous_model
      - checkpointing
      - optimization_criterion
      - optimizers
  data:
    type: object
    description: |
      Dataset configuration
    properties:
      dataset_id:
        type: string
        description: |
          ID of dataset to be used for training.
      bytes_parser_function:
        type: string
        description: |
          Function used to convert bytes received from the Storage, to a format useful for further transformations (e.g. Tensors).
          This function is called before any other transformations are performed on the data.
      transformations:
        type: array
        description: |
          Further (optional) transformations to be applied on the data after bytes_parser_function has been applied.
          For example, this can be torchvision transformations.
      label_transformer_function:
        type: string
        description: |
          (Optional) function used to transform the label (tensors of integers).
    required:
      - dataset_id
      - bytes_parser_function
  trigger:
    type: object
    description: |
      Defines the trigger to be used for training.
    properties:
      id:
        type: string
        description: |
          Datatype of concrete trigger to be used.
      trigger_config:
        type: object
        description:  |
          Configuration dictionary that will be passed to the trigger on initialization.
    required:
      - id
required:
  - pipeline
  - model
  - training
  - data
  - trigger
