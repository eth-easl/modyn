%YAML 1.1
---
$schema: "http://json-schema.org/draft-04/schema"
id: "http://stsci.edu/schemas/yaml-schema/draft-01"
title:
  Modyn Pipeline Configuration
description: |
  This is a configuration file for a pipeline to be run by Modyn.
properties:
  pipeline:
    type: object
    properties:
      name:
        type: string
        description: |
          The name of the pipeline.
      description:
        type: string
        description: |
          The description of the pipeline.
      version:
        type: string
        description: |
          The version of the pipeline.
    required:
      - name
  model:
    type: object
    properties:
      id:
        type: string
        description: |
          The ID of the model that should be trained.
      config:
        type: object
        description: |
          Configuration dictionary that will be passed to the model on initialization.
    required:
      - id
  training:
    type: object
    properties:
      gpus:
        type: number
        description: |
          The number of GPUs that should be used for training.
      epochs_per_trigger:
        type: number
        description: |
          The number of epochs per trigger. Defaults to 1, if not given.
      num_prefetched_partitions:
        type: number
        description: |
          The number of partitions that are prefetched per DataLoader worker. Defaults to 1, if not given.
      parallel_prefetch_requests:
        type: number
        description: |
          The number of parallel prefetch requests per DataLoader worker. Defaults to 1, if not given. Values bigger than num_prefetched_partitions are equal to num_prefetched_partitions. 
      device:
        type: string
        description: |
          The device the model should be put on. In the future (#131), we might want this to be either "cpu" or "gpu" and let the trainer server figure out the exact device, but for now, this really is the identifier of the device.
      amp:
        type: boolean
        description: |
          If True, automatic mixed precision will be used.
      dataloader_workers:
        type: number
        description: |
          The number of data loader workers on the trainer node that fetch data from storage.
      batch_size:
        type: number
        description: |
          The batch size to be used during training.
      use_previous_model:
        type: boolean
        description: |
          If True, on trigger, we continue training on the model outputted by the previous trigger. If False, we start with random weights.
          If `initial_model` is "pretrained", cannot be False.
      seed:
        type: number
        description: |
          If provided, every random python function (torch, numpy..) is seeded with this number. Must be in the range [0,100].
          Please be aware that if you want to seed postgres you must specify its seed in the modyn_config.
      selection_strategy:
        type: object
        description: |
          Configuration for the Selector
        properties:
          name:
            type: string
            description: |
              The Selector strategy that the pipeline should use. Currently supported: NewDataStrategy, FreshnessSamplingStrategy, CoresetStrategy
          maximum_keys_in_memory:
            type: number
            description: |
              Limits how many keys should be materialized at a time in the strategy.
          config:
            type: object
            description: |
              Configuration dictionary that will be passed to the strategy
            properties:
              limit:
                type: number
                description: |
                  This limits how many data points we train on at maximum on a trigger. Set to -1 to disable limit.
              uses_weights:
                type: boolean
                description: |
                  If set to true, weights are supplied from the selector and weighted SGD is used to train. By default, weights are ignored (uses_weights=False). Please note that this option is not related to Downsampling strategy, where weights are computed during training.
              reset_after_trigger:
                type: boolean
                description: |
                  If set to true, the selection strategy resets its internal state after a trigger.
              tail_triggers:
                type: number
                description: |
                  For the training iteration, just use data from this trigger and the previous tail_triggers. 
                  reset_after_trigger is equivalent to tail_triggers = 0. Omit this parameter if you want to use every previous datapoint.
              unused_data_ratio:
                type: number
                description: |
                 [FreshnessSamplingStrategy] Ratio that defines how much data in the training set per trigger should be from previously unused data (in all previous triggers).
              limit_reset:
                type: string
                description: |
                 [NewDataStrategy] Strategy to follow for respecting the limit in case of reset. Only used when reset_after_trigger == true. Either "lastX" or "sampleUAR" are supported. See class description for further info.
              presampling_config:
                type: object
                description: |
                  [CoresetStrategy] Config for the presampling strategy. If missing, no presampling is applied.
                properties:
                  strategy:
                    type: string
                    description: |
                      Strategy used to presample the data. Available strategies: Random, RandomNoReplacement, LabelBalanced, TriggerBalanced, No (get all data)
                  ratio:
                    type: number
                    description: |
                      Percentage of points on which the metric (loss, gradient norm,..) is computed. Must be between 0 and 100
              downsampling_config:
                description: |
                  You can either specify a single downsampler or a list of downsamplers. If you use the first option, 
                  please follow the format at the end of the schema (section definitions). The second option requires an 
                  array of downsamplers following the same format described before and a list of thresholds for the transitions.
                one of:
                  - $ref: "#/definitions/single_downsampling_config"
                  - type: object
                    properties:
                      downsampling_list:
                        type: array
                        items:
                          $ref: "#/definitions/single_downsampling_config"
                      downsampling_thresholds:
                        type: array
                        description: |
                          A list of thresholds to switch from a downsampler to another. The i-th threshold is used for the
                          transition from the i-th downsampler to the (i+1)-th. This array should have one less item on the list of downsamplers.
                        items:
                          type: int
          processor_type:
            type: string
            description: |
              The name of the Metadata Processor strategy that should be used.
        required:
          - name
          - maximum_keys_in_memory
      initial_model:
        type: string
        description: |
          What type of initial model should be used (random or pretrained).
      initial_model_id:
        type: number
        description: |
          In case of pretrained initial model, provide the model id of the initial model
      initial_pass:
        type: object
        description: |
          Setting of initial pass through data
        properties:
          activated:
            type: boolean
            description: |
              Whether we do an initial pass through the data
          reference:
            type: string
            description: |
              If we do an initial pass, we define whether we reference a certain percentage of all data (= amount) or all data in an interval (= start_timestamp, end_timestamp)
        required:
          - activated
      checkpointing:
        type: object
        description: |
          Configuration of checkpointing during training
        properties:
          activated:
            type: boolean
            description: |
              Whether we checkpoint or not
          interval:
            type: number
            description: |
              In what interval we checkpoint
          path:
            type: string
            description: |
              The path on the training node where the checkpoints are stored
        required:
          - activated
      optimizers:
        type: array
        description: |
          An array of the optimizers for the training
        minItems: 1
        items:
          type: object
          description: |
            Configuration for the optimizer (e.g., Adam)
          properties:
            name:
              type: string
              description: |
                The name of the optimizer (like an ID)
            algorithm:
              type: string
              description: |
                The type of the optimizer (e.g. SGD)
            source:
              type: string
              description: The framework/package the optimizer comes from. Currently PyTorch and APEX are supported
            param_groups:
              type: array
              description: |
                An array of the parameter groups (parameters and optional configs) that this optimizer is responsible for
              minItems: 1
              items:
                type: object
                description: |
                  Configuration for a parameter group
                properties:
                  module:
                    type: string
                    description: |
                      A set of parameters
                  config:
                    type: object
                    description: |
                      Optional configuration for the parameter group (e.g. learning rate)
                required:
                  - module
          required:
            - name
            - algorithm
            - source
            - param_groups
      optimization_criterion:
        type: object
        description: |
          Configuration for the optimization criterion that we optimize
        properties:
          name:
            type: string
            description: |
              The name of the criterion that the pipeline uses (e.g., CrossEntropyLoss)
          config:
            type: object
            description: |
              Optional configuration of the criterion. Passed to the optimizer class as a dict.
        required:
          - name
      lr_scheduler:
        type: object
        description: |
          Configuration for the Torch-based Learning Rate (LR) scheduler used for training.
        properties:
          name:
            type: string
            description: The name of the LR scheduler.
          source:
            type: string
            description: Source of the LR scheduler (for now, only PyTorch and custom are supported).
          optimizers:
            type: array
            minItems: 1
            description: |
              List of optimizers that this scheduler is responsible for. In case a PyTorch LR scheduler is used, this list should have only one item.
            items:
              type: string
          config:
            type: object
            description: |
              Optional configuration of the lr scheduler. Passed to the lr scheduler as a dict.
        required:
          - name
          - source
          - optimizers
      grad_scaler_config:
        type: object
        description: |
          Configuration for the torch.cuda.amp.GradScaler. Effective only when amp is enabled.
    required:
      - gpus
      - device
      - dataloader_workers
      - batch_size
      - selection_strategy
      - initial_model
      - initial_pass
      - use_previous_model
      - checkpointing
      - optimization_criterion
      - optimizers
  data:
    type: object
    description: |
      Dataset configuration
    properties:
      dataset_id:
        type: string
        description: |
          ID of dataset to be used for training.
      bytes_parser_function:
        type: string
        description: |
          Function used to convert bytes received from the Storage, to a format useful for further transformations (e.g. Tensors).
          This function is called before any other transformations are performed on the data.
      transformations:
        type: array
        description: |
          Further (optional) transformations to be applied on the data after bytes_parser_function has been applied.
          For example, this can be torchvision transformations.
      label_transformer_function:
        type: string
        description: |
          (Optional) function used to transform the label (tensors of integers).
      tokenizer:
        type: string
        description: |
          (Optional) Function to tokenize the input. Must be a class in modyn.models.tokenizers.
    required:
      - dataset_id
      - bytes_parser_function
  trigger:
    type: object
    description: |
      Defines the trigger to be used for training.
    properties:
      id:
        type: string
        description: |
          Datatype of concrete trigger to be used.
      trigger_config:
        type: object
        description:  |
          Configuration dictionary that will be passed to the trigger on initialization.
    required:
      - id
  evaluation:
    type: object
    description: |
      Evaluation configuration.
    properties:
      device:
        type: string
        description: |
          The device the model should be put on. In the future (#131), we might want this to be either "cpu" or "gpu"
          and let the evaluator figure out the exact device, but for now, this really is the identifier of the device.
      amp:
        type: boolean
        description: |
          If True, automatic mixed precision will be used.
      result_writers:
        type: array
        description: |
          Specifies in which formats to store the evaluation results. We currently support json and tensorboard.
        minItems: 1
        items:
          type: string
          description: |
            Name of the evaluation result writer.
      datasets:
        type: array
        description: |
          An array of all datasets on which the model is evaluated.
        minItems: 1
        items:
          type: object
          description: |
            Configuration for the evaluation dataset.
          properties:
            dataset_id:
              type: string
              description: |
                The id of the dataset.
            bytes_parser_function:
              type: string
              description: |
                Function used to convert bytes received from the storage, to a format useful for further transformations
                (e.g. Tensors). This function is called before any other transformations are performed on the data.
            transformations:
              type: array
              description: |
                Further (optional) transformations to be applied on the data after bytes_parser_function has been
                applied. For example, this can be torchvision transformations.
            label_transformer_function:
              type: string
              description: |
                (Optional) function used to transform the label (which are tensors of integers).
            batch_size:
              type: number
              description: |
                The batch size to be used during evaluation.
            dataloader_workers:
              type: number
              description: |
                The number of data loader workers on the evaluation node that fetch data from storage.
            metrics:
              type: array
              description: |
                All metrics used to evaluate the model on the given dataset.
              minItems: 1
              items:
                type: object
                description: |
                  Confguration for the metric.
                properties:
                  name:
                    type: string
                    description: |
                      The name of the evaluation metric. Currently we support F1-score, Accuracy and ROC-AUC.
                  config:
                    type: object
                    description: |
                      Optional configuration for the evaluation metric.
                  evaluation_transformer_function:
                    type: string
                    description: |
                      The (optional) function used to transform the model output before evaluation.
                      This might for example include the application of the sigmoid-function.
                required:
                  - name
          required:
            - dataset_id
            - bytes_parser_function
            - dataloader_workers
            - batch_size
            - metrics
    required:
      - device
      - result_writers
      - datasets
required:
  - pipeline
  - model
  - training
  - data
  - trigger
definitions:
  single_downsampling_config:
    type: object
    description: |
      [CoresetStrategy] Config for the downsampling strategy. If missing, No downsampling (NoDownsamplingStrategy) is applied.
    properties:
      strategy:
        type: string
        description: |
          Strategy used to downsample the data. Available strategies: Loss, Gradnorm, No (no downsampling)
      sample_then_batch:
        type: boolean
        description: |
          If True, the samples are first sampled and then batched and supplied to the training loop. If False, the datapoints are first divided into batches and then sampled.
      ratio:
        type: number
        description: |
          Ratio post_sampling_size/pre_sampling_size. For example, with 160 datapoints and a ratio of 50 we keep 80. Must be in (0,100].
      period:
        type: number
        description: |
          In multi-epoch training and sample_then_batch, how frequently the data is selected. The default value is 1 (select every epoch). To select once per trigger, set this parameter to 0.
    required:
      - strategy

