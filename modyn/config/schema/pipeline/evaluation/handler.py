from __future__ import annotations

from typing import Literal

from pydantic import Field, model_validator

from modyn.config.schema.base_model import ModynBaseModel

from .strategy import EvalStrategyConfig, OffsetEvalStrategyConfig

EvalHandlerExecutionTime = Literal["after_training", "after_pipeline", "manual"]
"""
after_training: will evaluate the models on the datasets after the training has finished
after_pipeline (prospective feature): will evaluate the models on the datasets after the pipeline has finished
manual (prospective feature): will skip the evaluations in the pipeline run; useful if someone wants to start them with
    a different entry point later on in isolation (useful for debugging and when evaluation is not part of the
    performance critical part of the pipeline, can e.g. be executed while the system is busy with other things)
async_during_pipeline (prospective feature): useful when the pipeline should perform evaluations asynchronously during
    the pipeline (e.g. for continuously generating data for training trigger decisions)
"""


class EvalHandlerConfig(ModynBaseModel):
    """Configuration of one certain evaluation sequence on modyn pipeline
    execution level.

    Configures what datasets a certain evaluation series should run on, what intervals and models to combine...
    This basically reduces the choices in the space `Datasets x Models x Intervals x TimeOfExecutionInPipeline`
    """

    name: str = Field(
        "Modyn EvalHandler", description="The name of the evaluation handler used to identify its outputs in the log"
    )

    execution_time: EvalHandlerExecutionTime = Field(
        description="When evaluations should be performed in terms of where in the code / when during a pipeline run."
    )
    """Caution: Using `execution_time=after_training` might lead to a slightly different behavior with respect to
    the `models` option compared to the `execution_time=after_pipeline`.

    Point of Difference: Determining the 'most recent' model for a given evaluation request.

    When `execution_time` is set to `after_pipeline`, all models generated within a certain evaluation request interval
    are considered. This allows the selection of the most recent model within that interval.
    However, when `execution_time` is set to `after_training`, the selection process is different. Models are generated
    during the pipeline run, and the most recently trained model may not necessarily be the most recent for the
    subsequent evaluation request. This is due to the possibility of another model being trained between the current
    training and the next evaluation interval. As a result, with `execution_time=after_training`, there could be
    multiple models deemed as 'most recent' for a given interval."""

    models: Literal["matrix", "active", "training"] = Field(
        "active",
        description="For a evaluation interval given by the strategy, this selects what models to evaluate there.",
    )
    """To give more context here, the matrix option takes a set of (potentially
    overlapping) intervals generated by the strategy and decides for every
    model generated over the course of a pipeline run, whether this model
    should be evaluated at this interval.

    The `active` option only evaluates the most recent model at each interval.
    `training` evaluates the latest model that is being trained in the current interval.
    Matrix will implement a cross product of models and intervals.
    This is conceptually independent from the `execution_time` of the evaluations. With `execution_time=after_training`
    we always have the most recent model available, but then have to decide for which intervals we want to evaluate it.
    When we have `execution_time=after_pipeline` we can take a different perspective as we have all models available
    already. For every interval we can simply find all intervals where this model is relevant and evaluate it there.
    In most cases this `is_relevant(mode, interval)` function will be a simple check whether a model is the most recent
    at a certain interval.
    """

    strategy: EvalStrategyConfig = Field(
        ..., description="Defining the strategy which obtains the intervals for the evaluation."
    )

    datasets: list[str] = Field(
        description=(
            "List of dataset references defined at top level of evaluation config, to enable running one "
            "handler on multiple datasets (e.g. train, test, combined). "
            "For `BetweenTwoTriggersEvalStrategy` we can e.g. run the handler on the training and test datasets."
        )
    )

    @model_validator(mode="after")
    def validate_datasets(self) -> EvalHandlerConfig:
        if isinstance(self.strategy, OffsetEvalStrategyConfig):
            assert self.execution_time == "after_training", "OffsetEvalStrategy can only be used after training."
            assert self.models == "train", "OffsetEvalStrategy can only be used with currently in training model."
        return self

    @model_validator(mode="after")
    def after_training_constraint(self) -> EvalHandlerConfig:
        # as we cannot define an evaluation interval's "active_model" property
        # in the `after_training` mode, we disable non-matrix evaluations in this mode.
        # This is because we need the training start timestamp for the next training
        # to determine the most recent model. We cannot use the training interval end
        # as dataset sparsity might lead to long periods between one training end and
        # the next training start.
        if self.execution_time == "after_training":
            assert (
                self.models == "matrix"
            ), "Non-matrix evaluations are not supported with after_training execution time."

        return self
