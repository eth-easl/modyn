from __future__ import annotations

from typing import List, Literal

from modyn.config.schema.base_model import ModynBaseModel
from pydantic import Field, model_validator

from .strategy import EvalStrategyConfig, OffsetEvalStrategyConfig

EvalHandlerExecutionTime = Literal["after_training"]
"""
after_training: will evaluate the models on the datasets after the training has finished
after_pipeline (prospective feature): will evaluate the models on the datasets after the pipeline has finished
manual (prospective feature): will skip the evaluations in the pipeline run; useful if someone wants to start them with
    a different entry point later on in isolation (useful for debugging and when evaluation is not part of the
    performance critical part of the pipeline, can e.g. be executed while the system is busy with other things)
async_during_pipeline (prospective feature): useful when the pipeline should perform evaluations asynchronously during
    the pipeline (e.g. for continuously generating data for training trigger decisions)
"""


class EvalHandlerConfig(ModynBaseModel):
    """Configuration of one certain evaluation sequence on modyn pipeline execution level.

    Configures what datasets a certain evaluation series should run on, what intervals and models to combine...
    This basically reduces the choices in the space `Datasets x Models x Intervals x TimeOfExecutionInPipeline`
    """

    name: str = Field(
        "Modyn EvalHandler", description="The name of the evaluation handler used to identify its outputs in the log"
    )

    execution_time: EvalHandlerExecutionTime = Field(
        description="When evaluations should be performed in terms of where in the code / when during a pipeline run."
    )
    """Caution: Using `execution_time=after_training` might lead to a slightly different behavior with respect to
    the `models` option compared to the `execution_time=after_pipeline`.

    Point of Difference: Determining the 'most recent' model for a given evaluation request.

    When `execution_time` is set to `after_pipeline`, all models generated within a certain evaluation request interval
    are considered. This allows the selection of the most recent model within that interval.
    However, when `execution_time` is set to `after_training`, the selection process is different. Models are generated
    during the pipeline run, and the most recently trained model may not necessarily be the most recent for the
    subsequent evaluation request. This is due to the possibility of another model being trained between the current
    training and the next evaluation interval. As a result, with `execution_time=after_training`, there could be
    multiple models deemed as 'most recent' for a given interval."""

    models: Literal["matrix", "most_recent"] = Field(
        "most_recent",
        description="For a evaluation interval given by the strategy, this selects what models to evaluate there.",
    )
    """
    To give more context here, the matrix option takes a set of (potentially overlapping) intervals generated by the
    strategy and decides for every model generated over the course of a pipeline run, whether this model should be
    evaluated at this interval. The `most_recent` option only evaluates the most recent model at each interval.
    Matrix will implement a cross product of models and intervals.
    This is conceptually independent from the `execution_time` of the evaluations. With `execution_time=after_training`
    we always have the most recent model available, but then have to decide for which intervals we want to evaluate it.
    When we have `execution_time=after_pipeline` we can take a different perspective as we have all models available
    already. For every interval we can simply find all intervals where this model is relevant and evaluate it there.
    In most cases this `is_relevant(mode, interval)` function will be a simple check whether a model is the most recent
    at a certain interval.
    """

    strategy: EvalStrategyConfig = Field(
        ..., description="Defining the strategy which obtains the intervals for the evaluation."
    )

    datasets: List[str] = Field(
        description=(
            "List of dataset references defined at top level of evaluation config, to enable running one "
            "handler on multiple datasets (e.g. train, test, combined). "
            "For `BetweenTwoTriggersEvalStrategy` we can e.g. run the handler on the training and test datasets."
        )
    )

    @model_validator(mode="after")
    def validate_datasets(self) -> EvalHandlerConfig:
        if isinstance(self.strategy, OffsetEvalStrategyConfig):
            assert self.execution_time == "after_training", "OffsetEvalStrategy can only be used after training."
            assert self.models == "most_recent", "OffsetEvalStrategy can only be used with most_recent model currently."
        return self
